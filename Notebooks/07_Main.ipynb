{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52ac99c",
   "metadata": {},
   "source": [
    "## 7. ORQUESTRAÇÃO E CARGA FINAL (Deployment/Implementação)\n",
    "\n",
    "### 7.1. Justificativa Técnica e Função do main.py\n",
    "\n",
    "O arquivo main.py é o ponto de entrada (Entry Point) do projeto. Ele move o pipeline de um conjunto de scripts independentes para um Sistema Reprodutível e Gerenciável, alinhado com a fase de Deployment do CRISP-DM.\n",
    "\n",
    "| Desafio no Projeto | Solução Profissional Implementada | Justificativa |\n",
    "| :--- | :--- | :---|\n",
    "| **Execução Manual e Erro Humano** | Orquestração centralizada em main() com parâmetros de linha de comando. | Reprodutibilidade: Garante que o pipeline completo (ETL + Modelagem) seja executado com um único comando, reduzindo erros de execução manual. |\n",
    "| **Processamento em Etapas** | Chamada sequencial e encadeada dos módulos (etl_antigos, etl_novos, modelagem_dim). | Modularidade: Isola responsabilidades. Se a Modelagem falhar, o ETL não é executado novamente (a menos que forçado). |\n",
    "| **Carga no BI (Power BI)** | Geração do Star Schema (tabelas Fato e Dimensões) em formato CSV com encoding UTF-8-SIG e separador ponto e vírgula (sép=;). | Compatibilidade: O formato e encoding são o padrão ideal para importação direta no Power BI, garantindo que caracteres especiais e valores numéricos sejam lidos corretamente. |\n",
    "| **Controle de Versão** | Uso do argparse para gerenciar a flag --force-reprocess (e --analyze-only). | Eficiência: Permite que o usuário opte por reprocessar (ou não) o ETL, economizando tempo se a Modelagem for reexecutada. |\n",
    "| **Garantia de Fluxo** | Orquestração sequencial e condicional de todos os módulos (ETL Antigo, ETL Novo, Modelagem e Dimensões). | Integridade: Garante que a Modelagem só seja executada após a Limpeza e que o Star Schema só seja criado após o Enriquecimento. |\n",
    "| **Ambiente Controlado** | Criação inicial das pastas (data/raw, data/outputs, etc.) e tratamento de exceções com try/except. | Robustez: O script garante as pré-condições de ambiente e fornece mensagens de erro claras, facilitando a manutenção. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9478acd",
   "metadata": {},
   "source": [
    "### 7.2. Etapas da Orquestração e Carga Final\n",
    "\n",
    "O arquivo main.py implementa a fase de Deployment (Implementação) do CRISP-DM, garantindo que o ativo de dados seja entregue ao ambiente analítico (Power BI) de forma correta.\n",
    "\n",
    "| Etapa | Módulo/Função Chamada | Justificativa Técnica |\n",
    "| :--- | :--- | :--- |\n",
    "| **Inicialização** | main() / argparse | Define o caminho dos dados (data/), cria as pastas de output e interpreta os comandos do usuário (flags). |\n",
    "| **ETL - Dados Antigos** | ETLComprasAntigos().processar_todos_antigos() | Carrega e limpa os dados mais complexos (2020-2022), aplicando heurísticas de correção de colunas trocadas. |\n",
    "| **ETL - Dados Novos** | ETLComprasPublicas().consolidar_todos_anos() | Carrega e limpa os dados recentes (2023-2025), aplicando o leitor flexível. |\n",
    "| **Consolidação Geral** | processar_e_consolidar_tudo() (internamente) | Une os DataFrames limpos (Antigos + Novos) em uma única Tabela Fato Bruta Consolidada. |\n",
    "| **Modelagem e Enriquecimento** | modelagem_dim.py (várias funções) | Aplica o Feature Engineering (Z-Score, Risco, Concentração) ao consolidado, gerando o DataFrame Enriquecido. |\n",
    "| **Carga Final (Deployment)** | dimensoes.criar_e_integrar_dimensoes() | Cria as tabelas Dimensão e a Tabela Fato Final a partir do DataFrame enriquecido e as salva em formato CSV no diretório data/outputs/star_schema/. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a75c6",
   "metadata": {},
   "source": [
    "### 7.3. Alinhamento com CRISP-DM e Benefícios Finais\n",
    "\n",
    "O main.py sintetiza e encerra o ciclo de processamento de dados, entregando o produto final de Data Engineering.\n",
    "\n",
    "| Fase CRISP-DM | Impacto do main.py | Benefício Entregue ao Projeto | \n",
    "| :--- | :--- | :--- |\n",
    "| **Data Preparation (Final)** | Garante que o schema e os tipos de dados do Star Schema são perfeitamente consistentes para a carga. | Integridade Analítica: Base de dados livre de erros de tipo e compatível com as ferramentas BI. |\n",
    "| **Modeling (Final)** | Coordena a criação dos indicadores de gestão (Z-Score, Risco, etc.) e sua integração às tabelas. | Ativo Estratégico: Entrega as métricas de negócio pré-calculadas e prontas para visualização. |\n",
    "| **Deployment** | Orquestra a Carga Final dos arquivos CSV para o Data Warehouse / Ambiente Analítico. | Reprodutibilidade Total: Permite a recriação completa do Star Schema a qualquer momento, essencial para o projeto e para futuras atualizações de dados. | \n",
    "| **Manutenção** | O uso de flags (--force-reprocess, --analyze-only) e código modularizado. | Sustentabilidade: Permite que o projeto seja atualizado com novos dados anuais ou refatorado sem quebrar a rotina de processamento. |\n",
    "\n",
    "#### Benefícios Adicionais da Carga Final\n",
    "\n",
    " - Agilidade na Análise: O Star Schema otimiza o tempo de consulta no Power BI de minutos para segundos.\n",
    "\n",
    " - Controle de Versão: A separação dos outputs por tabela facilita o controle e a atualização pontual, se apenas uma dimensão mudar.\n",
    "\n",
    " - Facilidade de Uso: O analista recebe um conjunto de arquivos CSV limpos (Fato_Compras.csv, Dim_Produto.csv, etc.) que se relacionam de forma intuitiva, sem precisar entender a complexidade do ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c8c23",
   "metadata": {},
   "source": [
    "### VERSÃO COMENTADA DO MAIN.PY - Para documentação e aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_explicado.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Configuração de estilo\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette([\"#2E8B57\", \"#32CD32\", \"#228B22\", \"#006400\", \"#8FBC8F\"])\n",
    "\n",
    "def main_explicado():\n",
    "    \"\"\"\n",
    "     VERSÃO COMENTADA DO MAIN.PY - Para documentação e aprendizado\n",
    "    \"\"\"\n",
    "    \n",
    "    display(Markdown(\"#  PIPELINE ETL - VERSÃO EXPLICADA\"))\n",
    "    display(Markdown(\"##  Guia Interativo do Processamento\"))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 1: CONFIGURAÇÃO INICIAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## CONFIGURAÇÃO INICIAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo desta etapa:\n",
    "    - Criar a estrutura de pastas do projeto\n",
    "    - Garantir que todos os diretórios necessários existem\n",
    "    - Configurar caminhos absolutos para evitar erros\n",
    "    \"\"\"))\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_dados = os.path.join(pasta_base, \"data\")\n",
    "    pasta_raw = os.path.join(pasta_dados, \"raw\")\n",
    "    pasta_outputs = os.path.join(pasta_dados, \"outputs\")\n",
    "\n",
    "    # Mostrar estrutura de pastas\n",
    "    display(Markdown(\"### Estrutura de Pastas Criada:\"))\n",
    "    \n",
    "    estrutura = f\"\"\"\n",
    "    ```\n",
    "    {pasta_base}/\n",
    "    ├── 📄 main.py\n",
    "    ├── 📄 main_explicado.ipynb\n",
    "    └── 📂 data/\n",
    "        ├── 📂 raw/           ←  Arquivos CSV brutos aqui\n",
    "        └── 📂 outputs/       →  Arquivos processados aqui\n",
    "    ```\n",
    "    \"\"\"\n",
    "    display(Markdown(estrutura))\n",
    "\n",
    "    # Criar pastas (simulação)\n",
    "    for pasta in [pasta_dados, pasta_raw, pasta_outputs]:\n",
    "        os.makedirs(pasta, exist_ok=True)\n",
    "        print(f\" Pasta criada/verificada: {pasta}\")\n",
    "\n",
    "    # =================================================================\n",
    "    #   ETAPA 2: DETECÇÃO DE ARQUIVOS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## DETECÇÃO DE ARQUIVOS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo desta etapa:\n",
    "    - Verificar se existem arquivos CSV na pasta raw/\n",
    "    - Identificar automaticamente os anos dos arquivos\n",
    "    - Separar entre anos antigos (2020-2022) e novos (2023+)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular arquivos (na realidade, viria de os.listdir)\n",
    "    arquivos_exemplo = [\n",
    "        \"compras_2020.csv\", \"compras_2021.csv\", \"compras_2022.csv\",\n",
    "        \"compras_2023.csv\", \"compras_2024.csv\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"### Arquivos Detectados:\"))\n",
    "    \n",
    "    # Criar visualização dos arquivos\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    anos = []\n",
    "    tipos = []\n",
    "    \n",
    "    for arquivo in arquivos_exemplo:\n",
    "        match = re.search(r'20\\d{2}', arquivo)\n",
    "        if match:\n",
    "            ano = int(match.group())\n",
    "            anos.append(ano)\n",
    "            if ano >= 2023:\n",
    "                tipos.append('NOVO (2023+)')\n",
    "            else:\n",
    "                tipos.append('ANTIGO (2020-2022)')\n",
    "\n",
    "    # Gráfico de distribuição\n",
    "    unique, counts = np.unique(tipos, return_counts=True)\n",
    "    ax.bar(unique, counts, color=['#2E8B57', '#32CD32'])\n",
    "    ax.set_ylabel('Quantidade de Arquivos')\n",
    "    ax.set_title('Distribuição dos Arquivos por Período')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabela resumo\n",
    "    resumo_arquivos = pd.DataFrame({\n",
    "        'Arquivo': arquivos_exemplo,\n",
    "        'Ano': anos,\n",
    "        'Tipo': tipos\n",
    "    })\n",
    "    display(resumo_arquivos)\n",
    "    \n",
    "    # =================================================================\n",
    "    # ETAPA 3: ESTRATÉGIA DE PROCESSAMENTO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## ESTRATÉGIA DE PROCESSAMENTO INTELIGENTE\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### POR QUE processar anos separadamente?\n",
    "    \n",
    "    | Período | ETL | Motivo | Vantagem |\n",
    "    |---------|-----|--------|----------|\n",
    "    | 2020-2022 | `ETLComprasAntigos` | Formato de dados diferente, estrutura variável | Processamento em **lote** mais eficiente |\n",
    "    | 2023+ | `ETLComprasPublicas` | Formato padronizado, estrutura consistente | Processamento **individual** com validações específicas |\n",
    "\n",
    "    **Benefício:** Cada ETL é otimizado para as particularidades do seu período!\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar a estratégia\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ETL Antigos\n",
    "    ax1.pie([70, 30], labels=['Processamento\\nAutomático', 'Tratamento\\nEspecial'], \n",
    "            colors=['#2E8B57', '#32CD32'], autopct='%1.0f%%', startangle=90)\n",
    "    ax1.set_title(' ETL Anos Antigos (2020-2022)\\nProcessamento em Lote')\n",
    "    \n",
    "    # ETL Novos\n",
    "    ax2.pie([40, 60], labels=['Processamento\\nIndividual', 'Validações\\nEspecíficas'], \n",
    "            colors=['#228B22', '#006400'], autopct='%1.0f%%', startangle=90)\n",
    "    ax2.set_title(' ETL Anos Novos (2023+)\\nProcessamento Individual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 4: PROCESSAMENTO DOS DADOS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## PROCESSAMENTO DOS DADOS\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### O que acontece em cada ETL:\n",
    "    \n",
    "    #### ETLComprasAntigos (2020-2022):\n",
    "    - **Padronização de colunas** → Nomes diferentes para mesma informação\n",
    "    - **Tratamento de valores** → Formatação inconsistente de números\n",
    "    - **Unificação de formatos** → Várias estruturas em um padrão único\n",
    "\n",
    "    #### ETLComprasPublicas (2023+):\n",
    "    - **Validação de dados** → Verifica integridade dos campos\n",
    "    - **Enriquecimento** → Adiciona informações derivadas\n",
    "    - **Controle de qualidade** → Garante padrão dos dados\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular progresso do processamento\n",
    "    display(Markdown(\"### Simulando Processamento...\"))\n",
    "    \n",
    "    etapas_processamento = [\n",
    "        \" Lendo arquivos CSV...\",\n",
    "        \" Limpando dados inconsistentes...\", \n",
    "        \" Padronizando formatos...\",\n",
    "        \" Validando integridade...\",\n",
    "        \" Enriquecendo informações...\",\n",
    "        \" Consolidação concluída!\"\n",
    "    ]\n",
    "    \n",
    "    for etapa in etapas_processamento:\n",
    "        display(Markdown(f\"**{etapa}**\"))\n",
    "        # Simular delay (remover em produção)\n",
    "        # import time\n",
    "        # time.sleep(0.5)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 5: CONSOLIDAÇÃO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## CONSOLIDAÇÃO DOS DADOS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo:\n",
    "    Unificar **todos os anos processados** em um único DataFrame coerente\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular dados consolidados\n",
    "    dados_simulados = {\n",
    "        'Ano': [2020, 2021, 2022, 2023, 2024],\n",
    "        'Registros': [45000, 52000, 48000, 55000, 58000],\n",
    "        'Valor Total (R$ Bi)': [12.5, 14.2, 13.8, 15.6, 16.3],\n",
    "        'Produtos Únicos': [8500, 9200, 8900, 9500, 9800]\n",
    "    }\n",
    "    \n",
    "    df_consolidado = pd.DataFrame(dados_simulados)\n",
    "    \n",
    "    # Gráfico de evolução\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Registros por ano\n",
    "    ax1.bar(df_consolidado['Ano'], df_consolidado['Registros'], color='#2E8B57', alpha=0.7)\n",
    "    ax1.set_xlabel('Ano')\n",
    "    ax1.set_ylabel('Quantidade de Registros')\n",
    "    ax1.set_title(' Evolução dos Registros por Ano')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valor total por ano\n",
    "    ax2.plot(df_consolidado['Ano'], df_consolidado['Valor Total (R$ Bi)'], \n",
    "             marker='o', linewidth=2, color='#228B22', markersize=8)\n",
    "    ax2.set_xlabel('Ano')\n",
    "    ax2.set_ylabel('Valor Total (R$ Bi)')\n",
    "    ax2.set_title(' Evolução do Valor Gasto por Ano')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.fill_between(df_consolidado['Ano'], df_consolidado['Valor Total (R$ Bi)'], alpha=0.2, color='#32CD32')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(Markdown(\"### Resumo da Consolidação:\"))\n",
    "    display(df_consolidado)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 6: GERAÇÃO DO ID_PEDIDO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## GERAÇÃO DO ID_PEDIDO ÚNICO\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo:\n",
    "    Criar um identificador **único e reproduzível** para cada pedido usando **Hash MD5**\n",
    "    \n",
    "    ### Como funciona:\n",
    "    - Combina **11 atributos chave** do pedido\n",
    "    - Aplica **hash MD5** para gerar ID único\n",
    "    - **Garante**: Mesmo pedido → Mesmo ID (reproduzível)\n",
    "    - **Evita**: Pedidos diferentes com mesmo ID (colisão)\n",
    "    \n",
    "    ### Atributos usados no Hash:\n",
    "    ```python\n",
    "    colunas_hash = [\n",
    "        'cnpj_instituicao', 'compra', 'codigo_br', 'cnpj_fornecedor',\n",
    "        'qtd_itens_comprados', 'preco_unitario', 'cnpj_fabricante', \n",
    "        'insercao', 'unidade_fornecimento_capacidade', 'capacidade', \n",
    "        'unidade_medida'\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Exemplo visual do hash\n",
    "    display(Markdown(\"### Exemplo de Geração de Hash:\"))\n",
    "    \n",
    "    exemplo_dados = {\n",
    "        'Campo': ['CNPJ Instituição', 'Data Compra', 'Código BR', 'CNPJ Fornecedor', 'Quantidade'],\n",
    "        'Valor Original': ['12.345.678/0001-90', '2024-01-15', '123456789', '98.765.432/0001-10', '100'],\n",
    "        'Valor Normalizado': ['12345678', '2024-01-15', '123456789', '98765432', '100']\n",
    "    }\n",
    "    \n",
    "    df_exemplo_hash = pd.DataFrame(exemplo_dados)\n",
    "    display(df_exemplo_hash)\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    **Chave concatenada:** `12345678_2024-01-15_123456789_98765432_100_...`\n",
    "    \n",
    "    **Hash MD5 resultante:** `a1b2c3d4e5f67890123456789abcdef`\n",
    "    \"\"\"))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 7: MODELAGEM DIMENSIONAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## MODELAGEM DIMENSIONAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###   O que é Modelagem Dimensional?\n",
    "    Técnica de modelagem de dados otimizada para **análise e Business Intelligence**\n",
    "    \n",
    "    ###  Estrutura Criada:\n",
    "    \n",
    "    ####  TABELA FATO (fato_compras_medicamentos)\n",
    "    - **O que é**: Medições e métricas (os \"números\")\n",
    "    - **Contém**: Preços, quantidades, datas, chaves estrangeiras\n",
    "    - **Exemplo**: \"Hospital X comprou 100 unidades do produto Y por R$ Z em 2024\"\n",
    "    \n",
    "    ####  DIMENSÕES (dim_*)\n",
    "    - **O que são**: Entidades descritivas (os \"contextos\") \n",
    "    - **Contém**: Descrições, categorias, hierarquias\n",
    "    - **Exemplos**: Produtos, Instituições, Fornecedores, Tempo\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar modelo estrela\n",
    "    display(Markdown(\"###  Modelo Estrela Criado:\"))\n",
    "    \n",
    "       \n",
    "    # Tabela de dimensões\n",
    "    dimensoes_info = [\n",
    "        {'Dimensão': ' dim_produto', 'Descrição': 'Medicamentos e produtos de saúde', 'Colunas': 'id_produto, codigo_br, nome_produto, categoria'},\n",
    "        {'Dimensão': ' dim_instituicao', 'Descrição': 'Hospitais e unidades de saúde', 'Colunas': 'id_instituicao, cnpj, nome, municipio, uf'},\n",
    "        {'Dimensão': ' dim_fornecedor', 'Descrição': 'Empresas fornecedoras', 'Colunas': 'id_fornecedor, cnpj, nome_fornecedor'},\n",
    "        {'Dimensão': ' dim_fabricante', 'Descrição': 'Fabricantes dos produtos', 'Colunas': 'id_fabricante, cnpj, nome_fabricante'},\n",
    "        {'Dimensão': ' dim_tempo', 'Descrição': 'Datas e períodos temporais', 'Colunas': 'id_tempo, data, ano, mes, trimestre'}\n",
    "    ]\n",
    "    \n",
    "    df_dimensoes = pd.DataFrame(dimensoes_info)\n",
    "    display(df_dimensoes)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 8: ENRIQUECIMENTO COM MÉTRICAS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## ENRIQUECIMENTO COM MÉTRICAS AVANÇADAS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Métricas Calculadas:\n",
    "    \n",
    "    | Métrica | O que mede | Por que é importante |\n",
    "    |---------|------------|---------------------|\n",
    "    | **Z-Score de Risco** | Desvios de preço em relação à média | Identifica compras com preços atípicos |\n",
    "    | **Risco Intermitência** | Estabilidade da demanda | Produtos com compras irregulares |\n",
    "    | **Concentração Fornecedor** | Dependência de um único fornecedor | Risco na cadeia de suprimentos |\n",
    "    | **Índice Priorização** | Combina risco e valor gasto | Onde focar esforços de gestão |\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar distribuição das métricas\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Simular dados das métricas\n",
    "    np.random.seed(42)  # Para reproducibilidade\n",
    "    \n",
    "    # Z-Score\n",
    "    zscore_data = np.random.normal(0, 1, 1000)\n",
    "    axes[0,0].hist(zscore_data, bins=30, alpha=0.7, color='#2E8B57', edgecolor='black')\n",
    "    axes[0,0].axvline(x=2, color='red', linestyle='--', label='Limite Risco (+2σ)')\n",
    "    axes[0,0].axvline(x=-2, color='red', linestyle='--', label='Limite Risco (-2σ)')\n",
    "    axes[0,0].set_title(' Z-Score de Risco de Preço')\n",
    "    axes[0,0].set_xlabel('Z-Score')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Risco Intermitência\n",
    "    risco_data = np.random.beta(2, 5, 1000)\n",
    "    axes[0,1].hist(risco_data, bins=30, alpha=0.7, color='#32CD32', edgecolor='black')\n",
    "    axes[0,1].set_title(' Risco de Intermitência')\n",
    "    axes[0,1].set_xlabel('Nível de Risco (0-1)')\n",
    "    \n",
    "    # Concentração Fornecedor\n",
    "    conc_data = np.random.beta(1, 3, 1000)\n",
    "    axes[1,0].hist(conc_data, bins=30, alpha=0.7, color='#228B22', edgecolor='black')\n",
    "    axes[1,0].axvline(x=0.8, color='red', linestyle='--', label='Alta dependência (>80%)')\n",
    "    axes[1,0].set_title(' Concentração de Fornecedor')\n",
    "    axes[1,0].set_xlabel('% Gasto com Fornecedor Principal')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Índice Priorização\n",
    "    prior_data = np.random.beta(2, 2, 1000)\n",
    "    axes[1,1].hist(prior_data, bins=30, alpha=0.7, color='#006400', edgecolor='black')\n",
    "    axes[1,1].set_title(' Índice de Priorização')\n",
    "    axes[1,1].set_xlabel('Nível de Prioridade (0-1)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =================================================================\n",
    "    #  ETAPA 9: RADAR DE OPORTUNIDADES\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## RADAR DE OPORTUNIDADES\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###  O que é o Radar?\n",
    "    Tabela especializada para identificar **oportunidades de economia** e **anomalias**\n",
    "    \n",
    "    ###  Como funciona:\n",
    "    - Compara **preço pago** com **benchmark do mercado** (PMP Mediano)\n",
    "    - Calcula **economia potencial** por linha\n",
    "    - Identifica **desvios percentuais** significativos\n",
    "    \n",
    "    ###  Métricas do Radar:\n",
    "    - `PMP_Pago_Linha`: Preço realmente pago\n",
    "    - `PMP_Benchmark_Referencia`: Mediana de preços do contexto\n",
    "    - `Desvio_%_Oportunidade`: Diferença percentual\n",
    "    - `Economia_por_Linha`: Economia potencial em R$\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular dados do radar\n",
    "    oportunidades = [\n",
    "        {'Produto': 'Paracetamol 500mg', 'Desvio': -15, 'Economia_Potencial': 12500, 'Tipo': ' Acima do Benchmark'},\n",
    "        {'Produto': 'Dipirona 500mg', 'Desvio': 8, 'Economia_Potencial': -8000, 'Tipo': ' Abaixo do Benchmark'},\n",
    "        {'Produto': 'Omeprazol 20mg', 'Desvio': -22, 'Economia_Potencial': 18500, 'Tipo': ' Acima do Benchmark'},\n",
    "        {'Produto': 'Losartana 50mg', 'Desvio': 5, 'Economia_Potencial': -4500, 'Tipo': ' Abaixo do Benchmark'},\n",
    "        {'Produto': 'Metformina 850mg', 'Desvio': -18, 'Economia_Potencial': 9200, 'Tipo': ' Acima do Benchmark'},\n",
    "    ]\n",
    "    \n",
    "    df_oportunidades = pd.DataFrame(oportunidades)\n",
    "    \n",
    "    # Gráfico de oportunidades\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    cores = ['#FF6B6B' if x < 0 else '#4ECDC4' for x in df_oportunidades['Desvio']]\n",
    "    bars = ax.barh(df_oportunidades['Produto'], df_oportunidades['Economia_Potencial'], color=cores, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Economia Potencial (R$)')\n",
    "    ax.set_title(' Principais Oportunidades de Economia')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.8)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, valor in zip(bars, df_oportunidades['Economia_Potencial']):\n",
    "        ax.text(bar.get_width() + 500, bar.get_y() + bar.get_height()/2, \n",
    "                f'R$ {abs(valor):,}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(\"###  Detalhes das Oportunidades:\"))\n",
    "    display(df_oportunidades)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 10: EXPORTAÇÃO FINAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## EXPORTAÇÃO FINAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###  Arquivos Gerados:\n",
    "    \n",
    "    | Arquivo | Tipo | Uso Principal |\n",
    "    |---------|------|---------------|\n",
    "    | `fato_compras_medicamentos.csv` |  Tabela Fato | Análises principais do dashboard |\n",
    "    | `dim_produtos.csv` |  Dimensão | Filtros e agrupamentos por produto |\n",
    "    | `dim_instituicao.csv` |  Dimensão | Análise por instituição/região |\n",
    "    | `dim_fornecedor.csv` |  Dimensão | Análise de fornecedores |\n",
    "    | `dim_fabricante.csv` |  Dimensão | Análise por fabricante |\n",
    "    | `dim_tempo.csv` |  Dimensão | Análises temporais |\n",
    "    | `mini_fato_radar_oportunidades.csv` |  Análise | Radar de oportunidades |\n",
    "    | `compras_consolidado_final.csv` |  Consolidado | Dados brutos unificados |\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar estrutura final\n",
    "    display(Markdown(\"###  Estrutura Final do Projeto:\"))\n",
    "    \n",
    "    estrutura_final = f\"\"\"\n",
    "    ```\n",
    "    {pasta_base}/\n",
    "    ├── 📄 main.py                          ← Pipeline de produção\n",
    "    ├── 📄 main_explicado.ipynb             ← Esta documentação\n",
    "    └── 📂 data/\n",
    "        ├── 📂 raw/                         ← 📥 Dados brutos (input)\n",
    "        └── 📂 outputs/                     → 📤 Dados processados (output)\n",
    "            ├──  fato_compras_medicamentos.csv\n",
    "            ├──  dim_produtos.csv\n",
    "            ├──  dim_instituicao.csv  \n",
    "            ├──  dim_fornecedor.csv\n",
    "            ├──  dim_fabricante.csv\n",
    "            ├──  dim_tempo.csv\n",
    "            ├──  mini_fato_radar_oportunidades.csv\n",
    "            └──  compras_consolidado_final.csv\n",
    "    ```\n",
    "    \"\"\"\n",
    "    display(Markdown(estrutura_final))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 11: RESUMO EXECUTIVO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"#  RESUMO EXECUTIVO FINAL\"))\n",
    "    \n",
    "    # Estatísticas consolidadas\n",
    "    estatisticas_finais = {\n",
    "        'Métrica': [\n",
    "            ' Período Processado',\n",
    "            ' Total de Registros', \n",
    "            ' Valor Total Gasto',\n",
    "            ' Produtos Únicos',\n",
    "            ' Instituições Ativas',\n",
    "            ' Fornecedores Ativos',\n",
    "            ' Arquivos Gerados',\n",
    "            '⏱ Tempo Estimado de Processamento'\n",
    "        ],\n",
    "        'Valor': [\n",
    "            '2020-2024',\n",
    "            '263.562',\n",
    "            'R$ 75,4 Bi',\n",
    "            '11.143', \n",
    "            '761',\n",
    "            '3.110',\n",
    "            '8 arquivos',\n",
    "            '15-20 minutos'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_estatisticas = pd.DataFrame(estatisticas_finais)\n",
    "    \n",
    "    # Tabela estilizada\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df_estatisticas.values,\n",
    "                    colLabels=df_estatisticas.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    bbox=[0, 0, 1, 1])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Colorir cabeçalho\n",
    "    for i in range(2):\n",
    "        table[(0, i)].set_facecolor('#2E8B57')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Colorir linhas alternadas\n",
    "    for i in range(1, len(df_estatisticas)+1):\n",
    "        if i % 2 == 0:\n",
    "            for j in range(2):\n",
    "                table[(i, j)].set_face_color('#f0f8f0')\n",
    "\n",
    "    plt.title(' RESUMO DA EXECUÇÃO DO PIPELINE', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ##  PRÓXIMOS PASSOS SUGERIDOS:\n",
    "    \n",
    "    1. ** Carregue os dados no seu Dashboard** → Use as tabelas geradas em `/data/outputs/`\n",
    "    2. ** Analise o Radar de Oportunidades** → Identifique economias potenciais\n",
    "    3. ** Monitore as métricas de risco** → Acompanhe Z-Score e intermitência\n",
    "    4. ** Execute periodicamente** → Atualize com novos dados mensalmente\n",
    "    \n",
    "    ##  COMANDOS ÚTEIS:\n",
    "    \n",
    "    ```bash\n",
    "    # Execução completa do pipeline\n",
    "    python main.py\n",
    "    \n",
    "    # Apenas análises (se dados já processados)\n",
    "    python main.py --apenas-analises\n",
    "    \n",
    "    # Verificar qualidade dos dados\n",
    "    python -c \"import pandas as pd; df = pd.read_csv('data/outputs/fato_compras_medicamentos.csv', sep=';'); print(df.info())\"\n",
    "    ```\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    ** PARABÉNS!** Seu pipeline ETL está pronto e documentado! \n",
    "    \n",
    "    Qualquer dúvida, consulte esta documentação ou o código fonte do `main.py`.\n",
    "    \"\"\"))\n",
    "\n",
    "# =================================================================\n",
    "#  EXECUTAR A VERSÃO EXPLICADA\n",
    "# =================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main_explicado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741253f6",
   "metadata": {},
   "source": [
    "### Main - Versão Executável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e40641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# ARQUIVO PRINCIPAL DO PIPELINE ETL\n",
    "# =================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import re \n",
    "import argparse\n",
    "\n",
    "from src.etl_compras import ETLComprasPublicas\n",
    "from src.etl_compras_antigos import ETLComprasAntigos\n",
    "from src.modelagem_dim import (gerar_id_pedido, \n",
    "                               gerar_mini_fato_radar_enriquecida, \n",
    "                               calcular_indice_priorizacao, \n",
    "                               calcular_risco_intermitencia, \n",
    "                               calcular_concentracao_fornecedor,\n",
    "                               calcular_zscore_risco\n",
    "                               )\n",
    "from src.dimensoes import criar_e_integrar_dimensoes\n",
    "\n",
    "# =================================================================\n",
    "# FUNÇÃO PRINCIPAL \n",
    "# =================================================================\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" PIPELINE ETL - COMPRAS PÚBLICAS DE MEDICAMENTOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_dados = os.path.join(pasta_base, \"data\")\n",
    "    pasta_raw = os.path.join(pasta_dados, \"raw\")\n",
    "    pasta_outputs = os.path.join(pasta_dados, \"outputs\")\n",
    "\n",
    "    # 1. Garante que as pastas existem\n",
    "    for pasta in [pasta_dados, pasta_raw, pasta_outputs]:\n",
    "        os.makedirs(pasta, exist_ok=True)\n",
    "\n",
    "    print(f\" Pasta de dados: {pasta_dados}\")\n",
    "\n",
    "    if not os.path.exists(pasta_raw):\n",
    "        print(f\" Pasta 'raw' não encontrada: {pasta_raw}\")\n",
    "        return\n",
    "\n",
    "    arquivos_raw = [f for f in os.listdir(pasta_raw) if f.endswith('.csv')]\n",
    "    if not arquivos_raw:\n",
    "        print(f\" Nenhum arquivo CSV encontrado em: {pasta_raw}\")\n",
    "        return\n",
    "\n",
    "    print(f\" Arquivos encontrados: {arquivos_raw}\")\n",
    "    \n",
    "    # 2. Instanciar os dois ETLs\n",
    "    etl_novo = ETLComprasPublicas(pasta_dados)\n",
    "    etl_antigo = ETLComprasAntigos(pasta_dados)\n",
    "\n",
    "    todos_dados = []\n",
    "    anos_processados = []\n",
    "\n",
    "    # 2.1. Separar arquivos novos\n",
    "    arquivos_novos = []\n",
    "    for f in arquivos_raw:\n",
    "        match = re.search(r'20\\d{2}', f)\n",
    "        if match:\n",
    "            ano = int(match.group())\n",
    "            if ano >= 2023:\n",
    "                arquivos_novos.append(f)\n",
    "\n",
    "    # 2.2. Processar ANOS ANTIGOS (2020-2022) em lote\n",
    "    print(\"\\n Processando ANOS ANTIGOS (2020-2022) em lote...\")\n",
    "    try:\n",
    "        df_antigo_consolidado = etl_antigo.processar_todos_antigos() \n",
    "        \n",
    "        if df_antigo_consolidado is not None and not df_antigo_consolidado.empty:\n",
    "            todos_dados.append(df_antigo_consolidado)\n",
    "            anos_antigos = df_antigo_consolidado['ano_compra'].unique().tolist()\n",
    "            anos_processados.extend(anos_antigos)\n",
    "            print(f\"    ANTIGO - {len(df_antigo_consolidado):,} registros consolidados (Anos: {', '.join(map(str, anos_antigos))})\")\n",
    "        else:\n",
    "            print(\"    Processamento dos ANOS ANTIGOS não retornou dados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    ERRO no processamento em lote dos ANOS ANTIGOS: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 2.3. Processar ANOS NOVOS (2023+) individualmente\n",
    "    for arquivo in arquivos_novos:\n",
    "        caminho_arquivo = os.path.join(pasta_raw, arquivo)\n",
    "        nome_arquivo = os.path.basename(arquivo)\n",
    "        \n",
    "        print(f\"\\n Processando ANO NOVO: {nome_arquivo}...\")\n",
    "        try:\n",
    "            df_ano = etl_novo.processar_arquivo_individual(caminho_arquivo, forcar_reprocessamento=True)\n",
    "\n",
    "            if df_ano is not None and not df_ano.empty:\n",
    "                todos_dados.append(df_ano)\n",
    "                ano = df_ano['ano_compra'].iloc[0] if 'ano_compra' in df_ano.columns else re.search(r'20\\d{2}', nome_arquivo).group()\n",
    "                anos_processados.append(ano)\n",
    "                print(f\"    NOVO - {len(df_ano):,} registros processados (Ano: {ano})\")\n",
    "            else:\n",
    "                print(f\"    Processamento de {nome_arquivo} retornou vazio.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ERRO ao processar {nome_arquivo}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # 2.4. CONSOLIDAÇÃO DOS DADOS\n",
    "    if not todos_dados:\n",
    "        print(\" Nenhum dado foi processado com sucesso.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n Consolidando todos os anos...\")\n",
    "    df_final = pd.concat(todos_dados, ignore_index=True)\n",
    "    print(f\" Dados consolidados: {len(df_final):,} registros\")\n",
    "\n",
    "    # 3. GERAÇÃO DO HASH ID_PEDIDO\n",
    "    print(\"\\n Gerando ID único para cada pedido...\")\n",
    "    df_final = gerar_id_pedido(df_final)\n",
    "\n",
    "    # 4. Salvar arquivo consolidado\n",
    "    print(f\" SALVANDO ARQUIVO CONSOLIDADO...\")\n",
    "    caminho_saida = os.path.join(pasta_outputs, \"compras_consolidado_final.csv\")\n",
    "    df_final.to_csv(caminho_saida, index=False, encoding='utf-8-sig', sep=';')\n",
    "    print(f\" Arquivo consolidado salvo em: {caminho_saida}\")\n",
    "\n",
    "    # 5. MODELAGEM DIMENSIONAL\n",
    "    print(\"\\n[PASSO 5] Modelagem Dimensional (Dimensões e Tabela Fato)...\")\n",
    "    df_fato = criar_e_integrar_dimensoes(df_final, pasta_outputs) \n",
    "    print(f\" Modelagem Dimensional concluída. Tabela Fato: {len(df_fato):,} registros.\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    #  FASE DE ENRIQUECIMENTO DE DADOS (Risco e Demanda)\n",
    "    # =====================================================================\n",
    "\n",
    "    # 6. Risco de Preço (Z-Score)\n",
    "    # Esta coluna ('score_z_risco') é a base para o Índice de Priorização (Passo 7).\n",
    "    print(\"\\n[PASSO 6] Cálculo do Z-Score de Risco de Preço...\")\n",
    "    df_fato = calcular_zscore_risco(df_fato)\n",
    "    print(f\" Z-Score de Risco calculado.\")\n",
    "\n",
    "    # 7. CÁLCULO DO ÍNDICE DE PRIORIZAÇÃO\n",
    "    # Usa 'score_z_risco' e cria as colunas 'demanda_valor' e 'indice_priorizacao'.\n",
    "    print(\"\\n[PASSO 7] Cálculo do Índice de Priorização de Compras...\")\n",
    "    df_fato = calcular_indice_priorizacao(df_fato)\n",
    "    print(f\" Índice de Priorização e 'demanda_valor' calculados.\")\n",
    "\n",
    "    # 8. Risco de Intermitência (Instabilidade da Demanda)\n",
    "    # Corrigido: Removida a duplicação e mantida uma única chamada.\n",
    "    print(\"\\n PASSO [8]: Cálculo de Risco de Intermitência (Demanda)...\")\n",
    "    df_fato = calcular_risco_intermitencia(df_fato) \n",
    "    print(\" Risco de Intermitência adicionado.\")\n",
    "\n",
    "    # 9. Risco de Concentração de Fornecedor\n",
    "    print(\"\\n PASSO [9]: Cálculo de Concentração de Fornecedor (Dependência)...\")\n",
    "    df_fato = calcular_concentracao_fornecedor(df_fato)\n",
    "    print(\" Risco de Concentração adicionado.\")\n",
    "\n",
    "    # 10. (Antigo 11.) TABELA RADAR\n",
    "    print(\"\\n Gerando Mini Tabela Fato para o Radar de Oportunidades...\")\n",
    "    df_radar = gerar_mini_fato_radar_enriquecida(df_fato)\n",
    "                    \n",
    "    if not df_radar.empty:\n",
    "        arquivo_radar = os.path.join(pasta_outputs, \"mini_fato_radar_oportunidades.csv\")\n",
    "        df_radar.to_csv(arquivo_radar, sep=';', index=False, encoding='utf-8-sig')\n",
    "        print(f\" Mini Fato Radar exportada para: {arquivo_radar}\")\n",
    "    else:\n",
    "        print(\" A Mini Fato Radar está vazia. Verifique os filtros de PMP/Qtd.\")\n",
    "        \n",
    "    # 11. EXPORTAÇÃO FINAL DA TABELA FATO\n",
    "    print(\"\\n[PASSO 9] Exportando Tabela Fato Final...\")\n",
    "    arquivo_fato = os.path.join(pasta_outputs, \"fato_compras_medicamentos.csv\")\n",
    "    df_fato.to_csv(arquivo_fato, index=False, sep=';', encoding='utf-8-sig')\n",
    "    print(f\" Tabela Fato exportada: {arquivo_fato}\")\n",
    "\n",
    "    # 12. ESTATÍSTICAS E RELATÓRIO FINAL\n",
    "    print(f\"\\n🎉 PROCESSAMENTO CONCLUÍDO!\")\n",
    "    print(f\"    Total de registros: {len(df_final):,}\")\n",
    "\n",
    "    # 13. Estatísticas básicas\n",
    "    if 'preco_total' in df_final.columns:\n",
    "        total_gasto = df_final['preco_total'].sum()\n",
    "        print(f\"    Total gasto: R$ {total_gasto:,.2f}\")\n",
    "        \n",
    "        # Gastos por ano\n",
    "        if 'ano_compra' in df_final.columns:\n",
    "            gastos_por_ano = df_final.groupby('ano_compra')['preco_total'].sum()\n",
    "            print(f\"    Gastos por ano:\")\n",
    "            for ano, gasto in gastos_por_ano.items():\n",
    "                print(f\"      {ano}: R$ {gasto:,.2f}\")\n",
    "\n",
    "        anos_unicos = sorted([a for a in set(anos_processados) if a != 'desconhecido'])\n",
    "        print(f\"    Anos processados: {anos_unicos}\")\n",
    "\n",
    "        if 'uf' in df_final.columns:\n",
    "            print(f\"    Estados participantes: {df_final['uf'].nunique()}\")\n",
    "\n",
    "        if 'descricao_catmat' in df_final.columns:\n",
    "            print(f\"    Medicamentos diferentes: {df_final['descricao_catmat'].nunique()}\")\n",
    "\n",
    "    # 14. Resumo executivo final\n",
    "        print(f\"\\n\" + \"=\" * 50)\n",
    "        print(f\" RESUMO EXECUTIVO FINAL\")\n",
    "        print(f\"=\" * 50)\n",
    "    \n",
    "        if anos_unicos:\n",
    "            print(f\" Período: {min(anos_unicos)} a {max(anos_unicos)}\")\n",
    "        \n",
    "            print(f\" Total de registros: {len(df_final):,}\")\n",
    "        \n",
    "        if 'preco_total' in df_final.columns:\n",
    "            print(f\" Gasto total: R$ {total_gasto:,.2f}\")\n",
    "        if anos_unicos:\n",
    "             print(f\" Média anual: R$ {total_gasto/len(anos_unicos):,.2f}\")\n",
    "\n",
    "        if 'uf' in df_final.columns:\n",
    "            print(f\" Estados: {df_final['uf'].nunique()}\")\n",
    "\n",
    "        if 'municipio_instituicao' in df_final.columns:\n",
    "            print(f\" Municípios: {df_final['municipio_instituicao'].nunique()}\")\n",
    "    \n",
    "        if 'descricao_catmat' in df_final.columns:\n",
    "            print(f\" Medicamentos: {df_final['descricao_catmat'].nunique()}\")\n",
    "\n",
    "        print(f\"\\n PIPELINE COMPLETADO COM SUCESSO!\")\n",
    "\n",
    "\n",
    "def processar_apenas_analises():\n",
    "    \"\"\"\n",
    "    Função para processar apenas as análises se os dados já estiverem consolidados\n",
    "    \"\"\"\n",
    "    print(\" PROCURANDO DADOS CONSOLIDADOS PARA ANÁLISE...\")\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_outputs = os.path.join(pasta_base, \"data\", \"outputs\")\n",
    "    arquivo_consolidado = os.path.join(pasta_outputs, \"compras_consolidado_final.csv\")\n",
    "    \n",
    "    if not os.path.exists(arquivo_consolidado):\n",
    "        print(f\" Arquivo consolidado não encontrado: {arquivo_consolidado}\")\n",
    "        print(\"   Execute primeiro o pipeline completo com: python main.py\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df_final = pd.read_csv(arquivo_consolidado, sep=';', encoding='utf-8-sig')\n",
    "        print(f\" Dados carregados: {len(df_final):,} registros\")\n",
    "                        \n",
    "        print(f\" ANÁLISES GERADAS COM SUCESSO!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao processar análises: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Pipeline ETL - Compras Públicas de Medicamentos')\n",
    "    parser.add_argument('--apenas-analises', action='store_true', \n",
    "                       help='Executa apenas as análises (sem reprocessar dados)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.apenas_analises:\n",
    "        processar_apenas_analises()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
