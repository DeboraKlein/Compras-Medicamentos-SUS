{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52ac99c",
   "metadata": {},
   "source": [
    "## 7. ORQUESTRA√á√ÉO E CARGA FINAL (Deployment/Implementa√ß√£o)\n",
    "\n",
    "### 7.1. Justificativa T√©cnica e Fun√ß√£o do main.py\n",
    "\n",
    "O arquivo main.py √© o ponto de entrada (Entry Point) do projeto. Ele move o pipeline de um conjunto de scripts independentes para um Sistema Reprodut√≠vel e Gerenci√°vel, alinhado com a fase de Deployment do CRISP-DM.\n",
    "\n",
    "| Desafio no Projeto | Solu√ß√£o Profissional Implementada | Justificativa |\n",
    "| :--- | :--- | :---|\n",
    "| **Execu√ß√£o Manual e Erro Humano** | Orquestra√ß√£o centralizada em main() com par√¢metros de linha de comando. | Reprodutibilidade: Garante que o pipeline completo (ETL + Modelagem) seja executado com um √∫nico comando, reduzindo erros de execu√ß√£o manual. |\n",
    "| **Processamento em Etapas** | Chamada sequencial e encadeada dos m√≥dulos (etl_antigos, etl_novos, modelagem_dim). | Modularidade: Isola responsabilidades. Se a Modelagem falhar, o ETL n√£o √© executado novamente (a menos que for√ßado). |\n",
    "| **Carga no BI (Power BI)** | Gera√ß√£o do Star Schema (tabelas Fato e Dimens√µes) em formato CSV com encoding UTF-8-SIG e separador ponto e v√≠rgula (s√©p=;). | Compatibilidade: O formato e encoding s√£o o padr√£o ideal para importa√ß√£o direta no Power BI, garantindo que caracteres especiais e valores num√©ricos sejam lidos corretamente. |\n",
    "| **Controle de Vers√£o** | Uso do argparse para gerenciar a flag --force-reprocess (e --analyze-only). | Efici√™ncia: Permite que o usu√°rio opte por reprocessar (ou n√£o) o ETL, economizando tempo se a Modelagem for reexecutada. |\n",
    "| **Garantia de Fluxo** | Orquestra√ß√£o sequencial e condicional de todos os m√≥dulos (ETL Antigo, ETL Novo, Modelagem e Dimens√µes). | Integridade: Garante que a Modelagem s√≥ seja executada ap√≥s a Limpeza e que o Star Schema s√≥ seja criado ap√≥s o Enriquecimento. |\n",
    "| **Ambiente Controlado** | Cria√ß√£o inicial das pastas (data/raw, data/outputs, etc.) e tratamento de exce√ß√µes com try/except. | Robustez: O script garante as pr√©-condi√ß√µes de ambiente e fornece mensagens de erro claras, facilitando a manuten√ß√£o. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9478acd",
   "metadata": {},
   "source": [
    "### 7.2. Etapas da Orquestra√ß√£o e Carga Final\n",
    "\n",
    "O arquivo main.py implementa a fase de Deployment (Implementa√ß√£o) do CRISP-DM, garantindo que o ativo de dados seja entregue ao ambiente anal√≠tico (Power BI) de forma correta.\n",
    "\n",
    "| Etapa | M√≥dulo/Fun√ß√£o Chamada | Justificativa T√©cnica |\n",
    "| :--- | :--- | :--- |\n",
    "| **Inicializa√ß√£o** | main() / argparse | Define o caminho dos dados (data/), cria as pastas de output e interpreta os comandos do usu√°rio (flags). |\n",
    "| **ETL - Dados Antigos** | ETLComprasAntigos().processar_todos_antigos() | Carrega e limpa os dados mais complexos (2020-2022), aplicando heur√≠sticas de corre√ß√£o de colunas trocadas. |\n",
    "| **ETL - Dados Novos** | ETLComprasPublicas().consolidar_todos_anos() | Carrega e limpa os dados recentes (2023-2025), aplicando o leitor flex√≠vel. |\n",
    "| **Consolida√ß√£o Geral** | processar_e_consolidar_tudo() (internamente) | Une os DataFrames limpos (Antigos + Novos) em uma √∫nica Tabela Fato Bruta Consolidada. |\n",
    "| **Modelagem e Enriquecimento** | modelagem_dim.py (v√°rias fun√ß√µes) | Aplica o Feature Engineering (Z-Score, Risco, Concentra√ß√£o) ao consolidado, gerando o DataFrame Enriquecido. |\n",
    "| **Carga Final (Deployment)** | dimensoes.criar_e_integrar_dimensoes() | Cria as tabelas Dimens√£o e a Tabela Fato Final a partir do DataFrame enriquecido e as salva em formato CSV no diret√≥rio data/outputs/star_schema/. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a75c6",
   "metadata": {},
   "source": [
    "### 7.3. Alinhamento com CRISP-DM e Benef√≠cios Finais\n",
    "\n",
    "O main.py sintetiza e encerra o ciclo de processamento de dados, entregando o produto final de Data Engineering.\n",
    "\n",
    "| Fase CRISP-DM | Impacto do main.py | Benef√≠cio Entregue ao Projeto | \n",
    "| :--- | :--- | :--- |\n",
    "| **Data Preparation (Final)** | Garante que o schema e os tipos de dados do Star Schema s√£o perfeitamente consistentes para a carga. | Integridade Anal√≠tica: Base de dados livre de erros de tipo e compat√≠vel com as ferramentas BI. |\n",
    "| **Modeling (Final)** | Coordena a cria√ß√£o dos indicadores de gest√£o (Z-Score, Risco, etc.) e sua integra√ß√£o √†s tabelas. | Ativo Estrat√©gico: Entrega as m√©tricas de neg√≥cio pr√©-calculadas e prontas para visualiza√ß√£o. |\n",
    "| **Deployment** | Orquestra a Carga Final dos arquivos CSV para o Data Warehouse / Ambiente Anal√≠tico. | Reprodutibilidade Total: Permite a recria√ß√£o completa do Star Schema a qualquer momento, essencial para o projeto e para futuras atualiza√ß√µes de dados. | \n",
    "| **Manuten√ß√£o** | O uso de flags (--force-reprocess, --analyze-only) e c√≥digo modularizado. | Sustentabilidade: Permite que o projeto seja atualizado com novos dados anuais ou refatorado sem quebrar a rotina de processamento. |\n",
    "\n",
    "#### Benef√≠cios Adicionais da Carga Final\n",
    "\n",
    " - Agilidade na An√°lise: O Star Schema otimiza o tempo de consulta no Power BI de minutos para segundos.\n",
    "\n",
    " - Controle de Vers√£o: A separa√ß√£o dos outputs por tabela facilita o controle e a atualiza√ß√£o pontual, se apenas uma dimens√£o mudar.\n",
    "\n",
    " - Facilidade de Uso: O analista recebe um conjunto de arquivos CSV limpos (Fato_Compras.csv, Dim_Produto.csv, etc.) que se relacionam de forma intuitiva, sem precisar entender a complexidade do ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c8c23",
   "metadata": {},
   "source": [
    "### VERS√ÉO COMENTADA DO MAIN.PY - Para documenta√ß√£o e aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_explicado.ipynb\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Configura√ß√£o de estilo\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette([\"#2E8B57\", \"#32CD32\", \"#228B22\", \"#006400\", \"#8FBC8F\"])\n",
    "\n",
    "def main_explicado():\n",
    "    \"\"\"\n",
    "     VERS√ÉO COMENTADA DO MAIN.PY - Para documenta√ß√£o e aprendizado\n",
    "    \"\"\"\n",
    "    \n",
    "    display(Markdown(\"#  PIPELINE ETL - VERS√ÉO EXPLICADA\"))\n",
    "    display(Markdown(\"##  Guia Interativo do Processamento\"))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 1: CONFIGURA√á√ÉO INICIAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## CONFIGURA√á√ÉO INICIAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo desta etapa:\n",
    "    - Criar a estrutura de pastas do projeto\n",
    "    - Garantir que todos os diret√≥rios necess√°rios existem\n",
    "    - Configurar caminhos absolutos para evitar erros\n",
    "    \"\"\"))\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_dados = os.path.join(pasta_base, \"data\")\n",
    "    pasta_raw = os.path.join(pasta_dados, \"raw\")\n",
    "    pasta_outputs = os.path.join(pasta_dados, \"outputs\")\n",
    "\n",
    "    # Mostrar estrutura de pastas\n",
    "    display(Markdown(\"### Estrutura de Pastas Criada:\"))\n",
    "    \n",
    "    estrutura = f\"\"\"\n",
    "    ```\n",
    "    {pasta_base}/\n",
    "    ‚îú‚îÄ‚îÄ üìÑ main.py\n",
    "    ‚îú‚îÄ‚îÄ üìÑ main_explicado.ipynb\n",
    "    ‚îî‚îÄ‚îÄ üìÇ data/\n",
    "        ‚îú‚îÄ‚îÄ üìÇ raw/           ‚Üê  Arquivos CSV brutos aqui\n",
    "        ‚îî‚îÄ‚îÄ üìÇ outputs/       ‚Üí  Arquivos processados aqui\n",
    "    ```\n",
    "    \"\"\"\n",
    "    display(Markdown(estrutura))\n",
    "\n",
    "    # Criar pastas (simula√ß√£o)\n",
    "    for pasta in [pasta_dados, pasta_raw, pasta_outputs]:\n",
    "        os.makedirs(pasta, exist_ok=True)\n",
    "        print(f\" Pasta criada/verificada: {pasta}\")\n",
    "\n",
    "    # =================================================================\n",
    "    #   ETAPA 2: DETEC√á√ÉO DE ARQUIVOS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## DETEC√á√ÉO DE ARQUIVOS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo desta etapa:\n",
    "    - Verificar se existem arquivos CSV na pasta raw/\n",
    "    - Identificar automaticamente os anos dos arquivos\n",
    "    - Separar entre anos antigos (2020-2022) e novos (2023+)\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular arquivos (na realidade, viria de os.listdir)\n",
    "    arquivos_exemplo = [\n",
    "        \"compras_2020.csv\", \"compras_2021.csv\", \"compras_2022.csv\",\n",
    "        \"compras_2023.csv\", \"compras_2024.csv\"\n",
    "    ]\n",
    "    \n",
    "    display(Markdown(\"### Arquivos Detectados:\"))\n",
    "    \n",
    "    # Criar visualiza√ß√£o dos arquivos\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    anos = []\n",
    "    tipos = []\n",
    "    \n",
    "    for arquivo in arquivos_exemplo:\n",
    "        match = re.search(r'20\\d{2}', arquivo)\n",
    "        if match:\n",
    "            ano = int(match.group())\n",
    "            anos.append(ano)\n",
    "            if ano >= 2023:\n",
    "                tipos.append('NOVO (2023+)')\n",
    "            else:\n",
    "                tipos.append('ANTIGO (2020-2022)')\n",
    "\n",
    "    # Gr√°fico de distribui√ß√£o\n",
    "    unique, counts = np.unique(tipos, return_counts=True)\n",
    "    ax.bar(unique, counts, color=['#2E8B57', '#32CD32'])\n",
    "    ax.set_ylabel('Quantidade de Arquivos')\n",
    "    ax.set_title('Distribui√ß√£o dos Arquivos por Per√≠odo')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabela resumo\n",
    "    resumo_arquivos = pd.DataFrame({\n",
    "        'Arquivo': arquivos_exemplo,\n",
    "        'Ano': anos,\n",
    "        'Tipo': tipos\n",
    "    })\n",
    "    display(resumo_arquivos)\n",
    "    \n",
    "    # =================================================================\n",
    "    # ETAPA 3: ESTRAT√âGIA DE PROCESSAMENTO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## ESTRAT√âGIA DE PROCESSAMENTO INTELIGENTE\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### POR QUE processar anos separadamente?\n",
    "    \n",
    "    | Per√≠odo | ETL | Motivo | Vantagem |\n",
    "    |---------|-----|--------|----------|\n",
    "    | 2020-2022 | `ETLComprasAntigos` | Formato de dados diferente, estrutura vari√°vel | Processamento em **lote** mais eficiente |\n",
    "    | 2023+ | `ETLComprasPublicas` | Formato padronizado, estrutura consistente | Processamento **individual** com valida√ß√µes espec√≠ficas |\n",
    "\n",
    "    **Benef√≠cio:** Cada ETL √© otimizado para as particularidades do seu per√≠odo!\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar a estrat√©gia\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ETL Antigos\n",
    "    ax1.pie([70, 30], labels=['Processamento\\nAutom√°tico', 'Tratamento\\nEspecial'], \n",
    "            colors=['#2E8B57', '#32CD32'], autopct='%1.0f%%', startangle=90)\n",
    "    ax1.set_title(' ETL Anos Antigos (2020-2022)\\nProcessamento em Lote')\n",
    "    \n",
    "    # ETL Novos\n",
    "    ax2.pie([40, 60], labels=['Processamento\\nIndividual', 'Valida√ß√µes\\nEspec√≠ficas'], \n",
    "            colors=['#228B22', '#006400'], autopct='%1.0f%%', startangle=90)\n",
    "    ax2.set_title(' ETL Anos Novos (2023+)\\nProcessamento Individual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 4: PROCESSAMENTO DOS DADOS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## PROCESSAMENTO DOS DADOS\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### O que acontece em cada ETL:\n",
    "    \n",
    "    #### ETLComprasAntigos (2020-2022):\n",
    "    - **Padroniza√ß√£o de colunas** ‚Üí Nomes diferentes para mesma informa√ß√£o\n",
    "    - **Tratamento de valores** ‚Üí Formata√ß√£o inconsistente de n√∫meros\n",
    "    - **Unifica√ß√£o de formatos** ‚Üí V√°rias estruturas em um padr√£o √∫nico\n",
    "\n",
    "    #### ETLComprasPublicas (2023+):\n",
    "    - **Valida√ß√£o de dados** ‚Üí Verifica integridade dos campos\n",
    "    - **Enriquecimento** ‚Üí Adiciona informa√ß√µes derivadas\n",
    "    - **Controle de qualidade** ‚Üí Garante padr√£o dos dados\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular progresso do processamento\n",
    "    display(Markdown(\"### Simulando Processamento...\"))\n",
    "    \n",
    "    etapas_processamento = [\n",
    "        \" Lendo arquivos CSV...\",\n",
    "        \" Limpando dados inconsistentes...\", \n",
    "        \" Padronizando formatos...\",\n",
    "        \" Validando integridade...\",\n",
    "        \" Enriquecendo informa√ß√µes...\",\n",
    "        \" Consolida√ß√£o conclu√≠da!\"\n",
    "    ]\n",
    "    \n",
    "    for etapa in etapas_processamento:\n",
    "        display(Markdown(f\"**{etapa}**\"))\n",
    "        # Simular delay (remover em produ√ß√£o)\n",
    "        # import time\n",
    "        # time.sleep(0.5)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 5: CONSOLIDA√á√ÉO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## CONSOLIDA√á√ÉO DOS DADOS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo:\n",
    "    Unificar **todos os anos processados** em um √∫nico DataFrame coerente\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular dados consolidados\n",
    "    dados_simulados = {\n",
    "        'Ano': [2020, 2021, 2022, 2023, 2024],\n",
    "        'Registros': [45000, 52000, 48000, 55000, 58000],\n",
    "        'Valor Total (R$ Bi)': [12.5, 14.2, 13.8, 15.6, 16.3],\n",
    "        'Produtos √önicos': [8500, 9200, 8900, 9500, 9800]\n",
    "    }\n",
    "    \n",
    "    df_consolidado = pd.DataFrame(dados_simulados)\n",
    "    \n",
    "    # Gr√°fico de evolu√ß√£o\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Registros por ano\n",
    "    ax1.bar(df_consolidado['Ano'], df_consolidado['Registros'], color='#2E8B57', alpha=0.7)\n",
    "    ax1.set_xlabel('Ano')\n",
    "    ax1.set_ylabel('Quantidade de Registros')\n",
    "    ax1.set_title(' Evolu√ß√£o dos Registros por Ano')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valor total por ano\n",
    "    ax2.plot(df_consolidado['Ano'], df_consolidado['Valor Total (R$ Bi)'], \n",
    "             marker='o', linewidth=2, color='#228B22', markersize=8)\n",
    "    ax2.set_xlabel('Ano')\n",
    "    ax2.set_ylabel('Valor Total (R$ Bi)')\n",
    "    ax2.set_title(' Evolu√ß√£o do Valor Gasto por Ano')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.fill_between(df_consolidado['Ano'], df_consolidado['Valor Total (R$ Bi)'], alpha=0.2, color='#32CD32')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(Markdown(\"### Resumo da Consolida√ß√£o:\"))\n",
    "    display(df_consolidado)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 6: GERA√á√ÉO DO ID_PEDIDO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## GERA√á√ÉO DO ID_PEDIDO √öNICO\"))\n",
    "\n",
    "    display(Markdown(\"\"\"\n",
    "    ### Objetivo:\n",
    "    Criar um identificador **√∫nico e reproduz√≠vel** para cada pedido usando **Hash MD5**\n",
    "    \n",
    "    ### Como funciona:\n",
    "    - Combina **11 atributos chave** do pedido\n",
    "    - Aplica **hash MD5** para gerar ID √∫nico\n",
    "    - **Garante**: Mesmo pedido ‚Üí Mesmo ID (reproduz√≠vel)\n",
    "    - **Evita**: Pedidos diferentes com mesmo ID (colis√£o)\n",
    "    \n",
    "    ### Atributos usados no Hash:\n",
    "    ```python\n",
    "    colunas_hash = [\n",
    "        'cnpj_instituicao', 'compra', 'codigo_br', 'cnpj_fornecedor',\n",
    "        'qtd_itens_comprados', 'preco_unitario', 'cnpj_fabricante', \n",
    "        'insercao', 'unidade_fornecimento_capacidade', 'capacidade', \n",
    "        'unidade_medida'\n",
    "    ]\n",
    "    ```\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Exemplo visual do hash\n",
    "    display(Markdown(\"### Exemplo de Gera√ß√£o de Hash:\"))\n",
    "    \n",
    "    exemplo_dados = {\n",
    "        'Campo': ['CNPJ Institui√ß√£o', 'Data Compra', 'C√≥digo BR', 'CNPJ Fornecedor', 'Quantidade'],\n",
    "        'Valor Original': ['12.345.678/0001-90', '2024-01-15', '123456789', '98.765.432/0001-10', '100'],\n",
    "        'Valor Normalizado': ['12345678', '2024-01-15', '123456789', '98765432', '100']\n",
    "    }\n",
    "    \n",
    "    df_exemplo_hash = pd.DataFrame(exemplo_dados)\n",
    "    display(df_exemplo_hash)\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    **Chave concatenada:** `12345678_2024-01-15_123456789_98765432_100_...`\n",
    "    \n",
    "    **Hash MD5 resultante:** `a1b2c3d4e5f67890123456789abcdef`\n",
    "    \"\"\"))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 7: MODELAGEM DIMENSIONAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## MODELAGEM DIMENSIONAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###   O que √© Modelagem Dimensional?\n",
    "    T√©cnica de modelagem de dados otimizada para **an√°lise e Business Intelligence**\n",
    "    \n",
    "    ###  Estrutura Criada:\n",
    "    \n",
    "    ####  TABELA FATO (fato_compras_medicamentos)\n",
    "    - **O que √©**: Medi√ß√µes e m√©tricas (os \"n√∫meros\")\n",
    "    - **Cont√©m**: Pre√ßos, quantidades, datas, chaves estrangeiras\n",
    "    - **Exemplo**: \"Hospital X comprou 100 unidades do produto Y por R$ Z em 2024\"\n",
    "    \n",
    "    ####  DIMENS√ïES (dim_*)\n",
    "    - **O que s√£o**: Entidades descritivas (os \"contextos\") \n",
    "    - **Cont√©m**: Descri√ß√µes, categorias, hierarquias\n",
    "    - **Exemplos**: Produtos, Institui√ß√µes, Fornecedores, Tempo\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar modelo estrela\n",
    "    display(Markdown(\"###  Modelo Estrela Criado:\"))\n",
    "    \n",
    "       \n",
    "    # Tabela de dimens√µes\n",
    "    dimensoes_info = [\n",
    "        {'Dimens√£o': ' dim_produto', 'Descri√ß√£o': 'Medicamentos e produtos de sa√∫de', 'Colunas': 'id_produto, codigo_br, nome_produto, categoria'},\n",
    "        {'Dimens√£o': ' dim_instituicao', 'Descri√ß√£o': 'Hospitais e unidades de sa√∫de', 'Colunas': 'id_instituicao, cnpj, nome, municipio, uf'},\n",
    "        {'Dimens√£o': ' dim_fornecedor', 'Descri√ß√£o': 'Empresas fornecedoras', 'Colunas': 'id_fornecedor, cnpj, nome_fornecedor'},\n",
    "        {'Dimens√£o': ' dim_fabricante', 'Descri√ß√£o': 'Fabricantes dos produtos', 'Colunas': 'id_fabricante, cnpj, nome_fabricante'},\n",
    "        {'Dimens√£o': ' dim_tempo', 'Descri√ß√£o': 'Datas e per√≠odos temporais', 'Colunas': 'id_tempo, data, ano, mes, trimestre'}\n",
    "    ]\n",
    "    \n",
    "    df_dimensoes = pd.DataFrame(dimensoes_info)\n",
    "    display(df_dimensoes)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 8: ENRIQUECIMENTO COM M√âTRICAS\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## ENRIQUECIMENTO COM M√âTRICAS AVAN√áADAS\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ### M√©tricas Calculadas:\n",
    "    \n",
    "    | M√©trica | O que mede | Por que √© importante |\n",
    "    |---------|------------|---------------------|\n",
    "    | **Z-Score de Risco** | Desvios de pre√ßo em rela√ß√£o √† m√©dia | Identifica compras com pre√ßos at√≠picos |\n",
    "    | **Risco Intermit√™ncia** | Estabilidade da demanda | Produtos com compras irregulares |\n",
    "    | **Concentra√ß√£o Fornecedor** | Depend√™ncia de um √∫nico fornecedor | Risco na cadeia de suprimentos |\n",
    "    | **√çndice Prioriza√ß√£o** | Combina risco e valor gasto | Onde focar esfor√ßos de gest√£o |\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar distribui√ß√£o das m√©tricas\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Simular dados das m√©tricas\n",
    "    np.random.seed(42)  # Para reproducibilidade\n",
    "    \n",
    "    # Z-Score\n",
    "    zscore_data = np.random.normal(0, 1, 1000)\n",
    "    axes[0,0].hist(zscore_data, bins=30, alpha=0.7, color='#2E8B57', edgecolor='black')\n",
    "    axes[0,0].axvline(x=2, color='red', linestyle='--', label='Limite Risco (+2œÉ)')\n",
    "    axes[0,0].axvline(x=-2, color='red', linestyle='--', label='Limite Risco (-2œÉ)')\n",
    "    axes[0,0].set_title(' Z-Score de Risco de Pre√ßo')\n",
    "    axes[0,0].set_xlabel('Z-Score')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Risco Intermit√™ncia\n",
    "    risco_data = np.random.beta(2, 5, 1000)\n",
    "    axes[0,1].hist(risco_data, bins=30, alpha=0.7, color='#32CD32', edgecolor='black')\n",
    "    axes[0,1].set_title(' Risco de Intermit√™ncia')\n",
    "    axes[0,1].set_xlabel('N√≠vel de Risco (0-1)')\n",
    "    \n",
    "    # Concentra√ß√£o Fornecedor\n",
    "    conc_data = np.random.beta(1, 3, 1000)\n",
    "    axes[1,0].hist(conc_data, bins=30, alpha=0.7, color='#228B22', edgecolor='black')\n",
    "    axes[1,0].axvline(x=0.8, color='red', linestyle='--', label='Alta depend√™ncia (>80%)')\n",
    "    axes[1,0].set_title(' Concentra√ß√£o de Fornecedor')\n",
    "    axes[1,0].set_xlabel('% Gasto com Fornecedor Principal')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # √çndice Prioriza√ß√£o\n",
    "    prior_data = np.random.beta(2, 2, 1000)\n",
    "    axes[1,1].hist(prior_data, bins=30, alpha=0.7, color='#006400', edgecolor='black')\n",
    "    axes[1,1].set_title(' √çndice de Prioriza√ß√£o')\n",
    "    axes[1,1].set_xlabel('N√≠vel de Prioridade (0-1)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # =================================================================\n",
    "    #  ETAPA 9: RADAR DE OPORTUNIDADES\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## RADAR DE OPORTUNIDADES\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###  O que √© o Radar?\n",
    "    Tabela especializada para identificar **oportunidades de economia** e **anomalias**\n",
    "    \n",
    "    ###  Como funciona:\n",
    "    - Compara **pre√ßo pago** com **benchmark do mercado** (PMP Mediano)\n",
    "    - Calcula **economia potencial** por linha\n",
    "    - Identifica **desvios percentuais** significativos\n",
    "    \n",
    "    ###  M√©tricas do Radar:\n",
    "    - `PMP_Pago_Linha`: Pre√ßo realmente pago\n",
    "    - `PMP_Benchmark_Referencia`: Mediana de pre√ßos do contexto\n",
    "    - `Desvio_%_Oportunidade`: Diferen√ßa percentual\n",
    "    - `Economia_por_Linha`: Economia potencial em R$\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Simular dados do radar\n",
    "    oportunidades = [\n",
    "        {'Produto': 'Paracetamol 500mg', 'Desvio': -15, 'Economia_Potencial': 12500, 'Tipo': ' Acima do Benchmark'},\n",
    "        {'Produto': 'Dipirona 500mg', 'Desvio': 8, 'Economia_Potencial': -8000, 'Tipo': ' Abaixo do Benchmark'},\n",
    "        {'Produto': 'Omeprazol 20mg', 'Desvio': -22, 'Economia_Potencial': 18500, 'Tipo': ' Acima do Benchmark'},\n",
    "        {'Produto': 'Losartana 50mg', 'Desvio': 5, 'Economia_Potencial': -4500, 'Tipo': ' Abaixo do Benchmark'},\n",
    "        {'Produto': 'Metformina 850mg', 'Desvio': -18, 'Economia_Potencial': 9200, 'Tipo': ' Acima do Benchmark'},\n",
    "    ]\n",
    "    \n",
    "    df_oportunidades = pd.DataFrame(oportunidades)\n",
    "    \n",
    "    # Gr√°fico de oportunidades\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    cores = ['#FF6B6B' if x < 0 else '#4ECDC4' for x in df_oportunidades['Desvio']]\n",
    "    bars = ax.barh(df_oportunidades['Produto'], df_oportunidades['Economia_Potencial'], color=cores, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Economia Potencial (R$)')\n",
    "    ax.set_title(' Principais Oportunidades de Economia')\n",
    "    ax.axvline(x=0, color='black', linewidth=0.8)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, valor in zip(bars, df_oportunidades['Economia_Potencial']):\n",
    "        ax.text(bar.get_width() + 500, bar.get_y() + bar.get_height()/2, \n",
    "                f'R$ {abs(valor):,}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(\"###  Detalhes das Oportunidades:\"))\n",
    "    display(df_oportunidades)\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 10: EXPORTA√á√ÉO FINAL\n",
    "    # =================================================================\n",
    "    display(Markdown(\"## EXPORTA√á√ÉO FINAL\"))\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ###  Arquivos Gerados:\n",
    "    \n",
    "    | Arquivo | Tipo | Uso Principal |\n",
    "    |---------|------|---------------|\n",
    "    | `fato_compras_medicamentos.csv` |  Tabela Fato | An√°lises principais do dashboard |\n",
    "    | `dim_produtos.csv` |  Dimens√£o | Filtros e agrupamentos por produto |\n",
    "    | `dim_instituicao.csv` |  Dimens√£o | An√°lise por institui√ß√£o/regi√£o |\n",
    "    | `dim_fornecedor.csv` |  Dimens√£o | An√°lise de fornecedores |\n",
    "    | `dim_fabricante.csv` |  Dimens√£o | An√°lise por fabricante |\n",
    "    | `dim_tempo.csv` |  Dimens√£o | An√°lises temporais |\n",
    "    | `mini_fato_radar_oportunidades.csv` |  An√°lise | Radar de oportunidades |\n",
    "    | `compras_consolidado_final.csv` |  Consolidado | Dados brutos unificados |\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Visualizar estrutura final\n",
    "    display(Markdown(\"###  Estrutura Final do Projeto:\"))\n",
    "    \n",
    "    estrutura_final = f\"\"\"\n",
    "    ```\n",
    "    {pasta_base}/\n",
    "    ‚îú‚îÄ‚îÄ üìÑ main.py                          ‚Üê Pipeline de produ√ß√£o\n",
    "    ‚îú‚îÄ‚îÄ üìÑ main_explicado.ipynb             ‚Üê Esta documenta√ß√£o\n",
    "    ‚îî‚îÄ‚îÄ üìÇ data/\n",
    "        ‚îú‚îÄ‚îÄ üìÇ raw/                         ‚Üê üì• Dados brutos (input)\n",
    "        ‚îî‚îÄ‚îÄ üìÇ outputs/                     ‚Üí üì§ Dados processados (output)\n",
    "            ‚îú‚îÄ‚îÄ  fato_compras_medicamentos.csv\n",
    "            ‚îú‚îÄ‚îÄ  dim_produtos.csv\n",
    "            ‚îú‚îÄ‚îÄ  dim_instituicao.csv  \n",
    "            ‚îú‚îÄ‚îÄ  dim_fornecedor.csv\n",
    "            ‚îú‚îÄ‚îÄ  dim_fabricante.csv\n",
    "            ‚îú‚îÄ‚îÄ  dim_tempo.csv\n",
    "            ‚îú‚îÄ‚îÄ  mini_fato_radar_oportunidades.csv\n",
    "            ‚îî‚îÄ‚îÄ  compras_consolidado_final.csv\n",
    "    ```\n",
    "    \"\"\"\n",
    "    display(Markdown(estrutura_final))\n",
    "    \n",
    "    # =================================================================\n",
    "    #   ETAPA 11: RESUMO EXECUTIVO\n",
    "    # =================================================================\n",
    "    display(Markdown(\"#  RESUMO EXECUTIVO FINAL\"))\n",
    "    \n",
    "    # Estat√≠sticas consolidadas\n",
    "    estatisticas_finais = {\n",
    "        'M√©trica': [\n",
    "            ' Per√≠odo Processado',\n",
    "            ' Total de Registros', \n",
    "            ' Valor Total Gasto',\n",
    "            ' Produtos √önicos',\n",
    "            ' Institui√ß√µes Ativas',\n",
    "            ' Fornecedores Ativos',\n",
    "            ' Arquivos Gerados',\n",
    "            '‚è± Tempo Estimado de Processamento'\n",
    "        ],\n",
    "        'Valor': [\n",
    "            '2020-2024',\n",
    "            '263.562',\n",
    "            'R$ 75,4 Bi',\n",
    "            '11.143', \n",
    "            '761',\n",
    "            '3.110',\n",
    "            '8 arquivos',\n",
    "            '15-20 minutos'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_estatisticas = pd.DataFrame(estatisticas_finais)\n",
    "    \n",
    "    # Tabela estilizada\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df_estatisticas.values,\n",
    "                    colLabels=df_estatisticas.columns,\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    bbox=[0, 0, 1, 1])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Colorir cabe√ßalho\n",
    "    for i in range(2):\n",
    "        table[(0, i)].set_facecolor('#2E8B57')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Colorir linhas alternadas\n",
    "    for i in range(1, len(df_estatisticas)+1):\n",
    "        if i % 2 == 0:\n",
    "            for j in range(2):\n",
    "                table[(i, j)].set_face_color('#f0f8f0')\n",
    "\n",
    "    plt.title(' RESUMO DA EXECU√á√ÉO DO PIPELINE', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    display(Markdown(\"\"\"\n",
    "    ##  PR√ìXIMOS PASSOS SUGERIDOS:\n",
    "    \n",
    "    1. ** Carregue os dados no seu Dashboard** ‚Üí Use as tabelas geradas em `/data/outputs/`\n",
    "    2. ** Analise o Radar de Oportunidades** ‚Üí Identifique economias potenciais\n",
    "    3. ** Monitore as m√©tricas de risco** ‚Üí Acompanhe Z-Score e intermit√™ncia\n",
    "    4. ** Execute periodicamente** ‚Üí Atualize com novos dados mensalmente\n",
    "    \n",
    "    ##  COMANDOS √öTEIS:\n",
    "    \n",
    "    ```bash\n",
    "    # Execu√ß√£o completa do pipeline\n",
    "    python main.py\n",
    "    \n",
    "    # Apenas an√°lises (se dados j√° processados)\n",
    "    python main.py --apenas-analises\n",
    "    \n",
    "    # Verificar qualidade dos dados\n",
    "    python -c \"import pandas as pd; df = pd.read_csv('data/outputs/fato_compras_medicamentos.csv', sep=';'); print(df.info())\"\n",
    "    ```\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    ** PARAB√âNS!** Seu pipeline ETL est√° pronto e documentado! \n",
    "    \n",
    "    Qualquer d√∫vida, consulte esta documenta√ß√£o ou o c√≥digo fonte do `main.py`.\n",
    "    \"\"\"))\n",
    "\n",
    "# =================================================================\n",
    "#  EXECUTAR A VERS√ÉO EXPLICADA\n",
    "# =================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main_explicado()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741253f6",
   "metadata": {},
   "source": [
    "### Main - Vers√£o Execut√°vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e40641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# ARQUIVO PRINCIPAL DO PIPELINE ETL\n",
    "# =================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import re \n",
    "import argparse\n",
    "\n",
    "from src.etl_compras import ETLComprasPublicas\n",
    "from src.etl_compras_antigos import ETLComprasAntigos\n",
    "from src.modelagem_dim import (gerar_id_pedido, \n",
    "                               gerar_mini_fato_radar_enriquecida, \n",
    "                               calcular_indice_priorizacao, \n",
    "                               calcular_risco_intermitencia, \n",
    "                               calcular_concentracao_fornecedor,\n",
    "                               calcular_zscore_risco\n",
    "                               )\n",
    "from src.dimensoes import criar_e_integrar_dimensoes\n",
    "\n",
    "# =================================================================\n",
    "# FUN√á√ÉO PRINCIPAL \n",
    "# =================================================================\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\" PIPELINE ETL - COMPRAS P√öBLICAS DE MEDICAMENTOS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_dados = os.path.join(pasta_base, \"data\")\n",
    "    pasta_raw = os.path.join(pasta_dados, \"raw\")\n",
    "    pasta_outputs = os.path.join(pasta_dados, \"outputs\")\n",
    "\n",
    "    # 1. Garante que as pastas existem\n",
    "    for pasta in [pasta_dados, pasta_raw, pasta_outputs]:\n",
    "        os.makedirs(pasta, exist_ok=True)\n",
    "\n",
    "    print(f\" Pasta de dados: {pasta_dados}\")\n",
    "\n",
    "    if not os.path.exists(pasta_raw):\n",
    "        print(f\" Pasta 'raw' n√£o encontrada: {pasta_raw}\")\n",
    "        return\n",
    "\n",
    "    arquivos_raw = [f for f in os.listdir(pasta_raw) if f.endswith('.csv')]\n",
    "    if not arquivos_raw:\n",
    "        print(f\" Nenhum arquivo CSV encontrado em: {pasta_raw}\")\n",
    "        return\n",
    "\n",
    "    print(f\" Arquivos encontrados: {arquivos_raw}\")\n",
    "    \n",
    "    # 2. Instanciar os dois ETLs\n",
    "    etl_novo = ETLComprasPublicas(pasta_dados)\n",
    "    etl_antigo = ETLComprasAntigos(pasta_dados)\n",
    "\n",
    "    todos_dados = []\n",
    "    anos_processados = []\n",
    "\n",
    "    # 2.1. Separar arquivos novos\n",
    "    arquivos_novos = []\n",
    "    for f in arquivos_raw:\n",
    "        match = re.search(r'20\\d{2}', f)\n",
    "        if match:\n",
    "            ano = int(match.group())\n",
    "            if ano >= 2023:\n",
    "                arquivos_novos.append(f)\n",
    "\n",
    "    # 2.2. Processar ANOS ANTIGOS (2020-2022) em lote\n",
    "    print(\"\\n Processando ANOS ANTIGOS (2020-2022) em lote...\")\n",
    "    try:\n",
    "        df_antigo_consolidado = etl_antigo.processar_todos_antigos() \n",
    "        \n",
    "        if df_antigo_consolidado is not None and not df_antigo_consolidado.empty:\n",
    "            todos_dados.append(df_antigo_consolidado)\n",
    "            anos_antigos = df_antigo_consolidado['ano_compra'].unique().tolist()\n",
    "            anos_processados.extend(anos_antigos)\n",
    "            print(f\"    ANTIGO - {len(df_antigo_consolidado):,} registros consolidados (Anos: {', '.join(map(str, anos_antigos))})\")\n",
    "        else:\n",
    "            print(\"    Processamento dos ANOS ANTIGOS n√£o retornou dados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    ERRO no processamento em lote dos ANOS ANTIGOS: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 2.3. Processar ANOS NOVOS (2023+) individualmente\n",
    "    for arquivo in arquivos_novos:\n",
    "        caminho_arquivo = os.path.join(pasta_raw, arquivo)\n",
    "        nome_arquivo = os.path.basename(arquivo)\n",
    "        \n",
    "        print(f\"\\n Processando ANO NOVO: {nome_arquivo}...\")\n",
    "        try:\n",
    "            df_ano = etl_novo.processar_arquivo_individual(caminho_arquivo, forcar_reprocessamento=True)\n",
    "\n",
    "            if df_ano is not None and not df_ano.empty:\n",
    "                todos_dados.append(df_ano)\n",
    "                ano = df_ano['ano_compra'].iloc[0] if 'ano_compra' in df_ano.columns else re.search(r'20\\d{2}', nome_arquivo).group()\n",
    "                anos_processados.append(ano)\n",
    "                print(f\"    NOVO - {len(df_ano):,} registros processados (Ano: {ano})\")\n",
    "            else:\n",
    "                print(f\"    Processamento de {nome_arquivo} retornou vazio.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ERRO ao processar {nome_arquivo}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # 2.4. CONSOLIDA√á√ÉO DOS DADOS\n",
    "    if not todos_dados:\n",
    "        print(\" Nenhum dado foi processado com sucesso.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n Consolidando todos os anos...\")\n",
    "    df_final = pd.concat(todos_dados, ignore_index=True)\n",
    "    print(f\" Dados consolidados: {len(df_final):,} registros\")\n",
    "\n",
    "    # 3. GERA√á√ÉO DO HASH ID_PEDIDO\n",
    "    print(\"\\n Gerando ID √∫nico para cada pedido...\")\n",
    "    df_final = gerar_id_pedido(df_final)\n",
    "\n",
    "    # 4. Salvar arquivo consolidado\n",
    "    print(f\" SALVANDO ARQUIVO CONSOLIDADO...\")\n",
    "    caminho_saida = os.path.join(pasta_outputs, \"compras_consolidado_final.csv\")\n",
    "    df_final.to_csv(caminho_saida, index=False, encoding='utf-8-sig', sep=';')\n",
    "    print(f\" Arquivo consolidado salvo em: {caminho_saida}\")\n",
    "\n",
    "    # 5. MODELAGEM DIMENSIONAL\n",
    "    print(\"\\n[PASSO 5] Modelagem Dimensional (Dimens√µes e Tabela Fato)...\")\n",
    "    df_fato = criar_e_integrar_dimensoes(df_final, pasta_outputs) \n",
    "    print(f\" Modelagem Dimensional conclu√≠da. Tabela Fato: {len(df_fato):,} registros.\")\n",
    "    \n",
    "    # =====================================================================\n",
    "    #  FASE DE ENRIQUECIMENTO DE DADOS (Risco e Demanda)\n",
    "    # =====================================================================\n",
    "\n",
    "    # 6. Risco de Pre√ßo (Z-Score)\n",
    "    # Esta coluna ('score_z_risco') √© a base para o √çndice de Prioriza√ß√£o (Passo 7).\n",
    "    print(\"\\n[PASSO 6] C√°lculo do Z-Score de Risco de Pre√ßo...\")\n",
    "    df_fato = calcular_zscore_risco(df_fato)\n",
    "    print(f\" Z-Score de Risco calculado.\")\n",
    "\n",
    "    # 7. C√ÅLCULO DO √çNDICE DE PRIORIZA√á√ÉO\n",
    "    # Usa 'score_z_risco' e cria as colunas 'demanda_valor' e 'indice_priorizacao'.\n",
    "    print(\"\\n[PASSO 7] C√°lculo do √çndice de Prioriza√ß√£o de Compras...\")\n",
    "    df_fato = calcular_indice_priorizacao(df_fato)\n",
    "    print(f\" √çndice de Prioriza√ß√£o e 'demanda_valor' calculados.\")\n",
    "\n",
    "    # 8. Risco de Intermit√™ncia (Instabilidade da Demanda)\n",
    "    # Corrigido: Removida a duplica√ß√£o e mantida uma √∫nica chamada.\n",
    "    print(\"\\n PASSO [8]: C√°lculo de Risco de Intermit√™ncia (Demanda)...\")\n",
    "    df_fato = calcular_risco_intermitencia(df_fato) \n",
    "    print(\" Risco de Intermit√™ncia adicionado.\")\n",
    "\n",
    "    # 9. Risco de Concentra√ß√£o de Fornecedor\n",
    "    print(\"\\n PASSO [9]: C√°lculo de Concentra√ß√£o de Fornecedor (Depend√™ncia)...\")\n",
    "    df_fato = calcular_concentracao_fornecedor(df_fato)\n",
    "    print(\" Risco de Concentra√ß√£o adicionado.\")\n",
    "\n",
    "    # 10. (Antigo 11.) TABELA RADAR\n",
    "    print(\"\\n Gerando Mini Tabela Fato para o Radar de Oportunidades...\")\n",
    "    df_radar = gerar_mini_fato_radar_enriquecida(df_fato)\n",
    "                    \n",
    "    if not df_radar.empty:\n",
    "        arquivo_radar = os.path.join(pasta_outputs, \"mini_fato_radar_oportunidades.csv\")\n",
    "        df_radar.to_csv(arquivo_radar, sep=';', index=False, encoding='utf-8-sig')\n",
    "        print(f\" Mini Fato Radar exportada para: {arquivo_radar}\")\n",
    "    else:\n",
    "        print(\" A Mini Fato Radar est√° vazia. Verifique os filtros de PMP/Qtd.\")\n",
    "        \n",
    "    # 11. EXPORTA√á√ÉO FINAL DA TABELA FATO\n",
    "    print(\"\\n[PASSO 9] Exportando Tabela Fato Final...\")\n",
    "    arquivo_fato = os.path.join(pasta_outputs, \"fato_compras_medicamentos.csv\")\n",
    "    df_fato.to_csv(arquivo_fato, index=False, sep=';', encoding='utf-8-sig')\n",
    "    print(f\" Tabela Fato exportada: {arquivo_fato}\")\n",
    "\n",
    "    # 12. ESTAT√çSTICAS E RELAT√ìRIO FINAL\n",
    "    print(f\"\\nüéâ PROCESSAMENTO CONCLU√çDO!\")\n",
    "    print(f\"    Total de registros: {len(df_final):,}\")\n",
    "\n",
    "    # 13. Estat√≠sticas b√°sicas\n",
    "    if 'preco_total' in df_final.columns:\n",
    "        total_gasto = df_final['preco_total'].sum()\n",
    "        print(f\"    Total gasto: R$ {total_gasto:,.2f}\")\n",
    "        \n",
    "        # Gastos por ano\n",
    "        if 'ano_compra' in df_final.columns:\n",
    "            gastos_por_ano = df_final.groupby('ano_compra')['preco_total'].sum()\n",
    "            print(f\"    Gastos por ano:\")\n",
    "            for ano, gasto in gastos_por_ano.items():\n",
    "                print(f\"      {ano}: R$ {gasto:,.2f}\")\n",
    "\n",
    "        anos_unicos = sorted([a for a in set(anos_processados) if a != 'desconhecido'])\n",
    "        print(f\"    Anos processados: {anos_unicos}\")\n",
    "\n",
    "        if 'uf' in df_final.columns:\n",
    "            print(f\"    Estados participantes: {df_final['uf'].nunique()}\")\n",
    "\n",
    "        if 'descricao_catmat' in df_final.columns:\n",
    "            print(f\"    Medicamentos diferentes: {df_final['descricao_catmat'].nunique()}\")\n",
    "\n",
    "    # 14. Resumo executivo final\n",
    "        print(f\"\\n\" + \"=\" * 50)\n",
    "        print(f\" RESUMO EXECUTIVO FINAL\")\n",
    "        print(f\"=\" * 50)\n",
    "    \n",
    "        if anos_unicos:\n",
    "            print(f\" Per√≠odo: {min(anos_unicos)} a {max(anos_unicos)}\")\n",
    "        \n",
    "            print(f\" Total de registros: {len(df_final):,}\")\n",
    "        \n",
    "        if 'preco_total' in df_final.columns:\n",
    "            print(f\" Gasto total: R$ {total_gasto:,.2f}\")\n",
    "        if anos_unicos:\n",
    "             print(f\" M√©dia anual: R$ {total_gasto/len(anos_unicos):,.2f}\")\n",
    "\n",
    "        if 'uf' in df_final.columns:\n",
    "            print(f\" Estados: {df_final['uf'].nunique()}\")\n",
    "\n",
    "        if 'municipio_instituicao' in df_final.columns:\n",
    "            print(f\" Munic√≠pios: {df_final['municipio_instituicao'].nunique()}\")\n",
    "    \n",
    "        if 'descricao_catmat' in df_final.columns:\n",
    "            print(f\" Medicamentos: {df_final['descricao_catmat'].nunique()}\")\n",
    "\n",
    "        print(f\"\\n PIPELINE COMPLETADO COM SUCESSO!\")\n",
    "\n",
    "\n",
    "def processar_apenas_analises():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o para processar apenas as an√°lises se os dados j√° estiverem consolidados\n",
    "    \"\"\"\n",
    "    print(\" PROCURANDO DADOS CONSOLIDADOS PARA AN√ÅLISE...\")\n",
    "    \n",
    "    pasta_base = os.path.dirname(os.path.abspath(__file__))\n",
    "    pasta_outputs = os.path.join(pasta_base, \"data\", \"outputs\")\n",
    "    arquivo_consolidado = os.path.join(pasta_outputs, \"compras_consolidado_final.csv\")\n",
    "    \n",
    "    if not os.path.exists(arquivo_consolidado):\n",
    "        print(f\" Arquivo consolidado n√£o encontrado: {arquivo_consolidado}\")\n",
    "        print(\"   Execute primeiro o pipeline completo com: python main.py\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df_final = pd.read_csv(arquivo_consolidado, sep=';', encoding='utf-8-sig')\n",
    "        print(f\" Dados carregados: {len(df_final):,} registros\")\n",
    "                        \n",
    "        print(f\" AN√ÅLISES GERADAS COM SUCESSO!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao processar an√°lises: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Pipeline ETL - Compras P√∫blicas de Medicamentos')\n",
    "    parser.add_argument('--apenas-analises', action='store_true', \n",
    "                       help='Executa apenas as an√°lises (sem reprocessar dados)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.apenas_analises:\n",
    "        processar_apenas_analises()\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
