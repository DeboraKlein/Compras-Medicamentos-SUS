# =================================================================
# ARQUIVO PRINCIPAL DO PIPELINE ETL
# =================================================================
import os
import pandas as pd
import traceback
import re 
import argparse

from src.etl_compras import ETLComprasPublicas
from src.etl_compras_antigos import ETLComprasAntigos
from src.modelagem_dim import (gerar_id_pedido, 
                               gerar_mini_fato_radar_enriquecida, 
                               calcular_indice_priorizacao, 
                               calcular_risco_intermitencia, 
                               calcular_concentracao_fornecedor,
                               calcular_zscore_risco
                               )
from src.dimensoes import criar_e_integrar_dimensoes

# =================================================================
# FUN√á√ÉO PRINCIPAL 
# =================================================================
def main():
    print("=" * 60)
    print("üè• PIPELINE ETL - COMPRAS P√öBLICAS DE MEDICAMENTOS")
    print("=" * 60)
    
    pasta_base = os.path.dirname(os.path.abspath(__file__))
    pasta_dados = os.path.join(pasta_base, "data")
    pasta_raw = os.path.join(pasta_dados, "raw")
    pasta_outputs = os.path.join(pasta_dados, "outputs")

    # 1. Garante que as pastas existem
    for pasta in [pasta_dados, pasta_raw, pasta_outputs]:
        os.makedirs(pasta, exist_ok=True)

    print(f"üìÅ Pasta de dados: {pasta_dados}")

    if not os.path.exists(pasta_raw):
        print(f"‚ùå Pasta 'raw' n√£o encontrada: {pasta_raw}")
        return

    arquivos_raw = [f for f in os.listdir(pasta_raw) if f.endswith('.csv')]
    if not arquivos_raw:
        print(f"‚ùå Nenhum arquivo CSV encontrado em: {pasta_raw}")
        return

    print(f"üìä Arquivos encontrados: {arquivos_raw}")
    
    # 2. Instanciar os dois ETLs
    etl_novo = ETLComprasPublicas(pasta_dados)
    etl_antigo = ETLComprasAntigos(pasta_dados)

    todos_dados = []
    anos_processados = []

    # 2.1. Separar arquivos novos
    arquivos_novos = []
    for f in arquivos_raw:
        match = re.search(r'20\d{2}', f)
        if match:
            ano = int(match.group())
            if ano >= 2023:
                arquivos_novos.append(f)

    # 2.2. Processar ANOS ANTIGOS (2020-2022) em lote
    print("\nüìÖ Processando ANOS ANTIGOS (2020-2022) em lote...")
    try:
        df_antigo_consolidado = etl_antigo.processar_todos_antigos() 
        
        if df_antigo_consolidado is not None and not df_antigo_consolidado.empty:
            todos_dados.append(df_antigo_consolidado)
            anos_antigos = df_antigo_consolidado['ano_compra'].unique().tolist()
            anos_processados.extend(anos_antigos)
            print(f"   ‚úÖ ANTIGO - {len(df_antigo_consolidado):,} registros consolidados (Anos: {', '.join(map(str, anos_antigos))})")
        else:
            print("   ‚ùå Processamento dos ANOS ANTIGOS n√£o retornou dados.")

    except Exception as e:
        print(f"   ‚ùå ERRO no processamento em lote dos ANOS ANTIGOS: {e}")
        traceback.print_exc()

    # 2.3. Processar ANOS NOVOS (2023+) individualmente
    for arquivo in arquivos_novos:
        caminho_arquivo = os.path.join(pasta_raw, arquivo)
        nome_arquivo = os.path.basename(arquivo)
        
        print(f"\nüìÖ Processando ANO NOVO: {nome_arquivo}...")
        try:
            df_ano = etl_novo.processar_arquivo_individual(caminho_arquivo, forcar_reprocessamento=True)

            if df_ano is not None and not df_ano.empty:
                todos_dados.append(df_ano)
                ano = df_ano['ano_compra'].iloc[0] if 'ano_compra' in df_ano.columns else re.search(r'20\d{2}', nome_arquivo).group()
                anos_processados.append(ano)
                print(f"   ‚úÖ NOVO - {len(df_ano):,} registros processados (Ano: {ano})")
            else:
                print(f"   ‚ùå Processamento de {nome_arquivo} retornou vazio.")

        except Exception as e:
            print(f"   ‚ùå ERRO ao processar {nome_arquivo}: {e}")
            traceback.print_exc()

    # 2.4. CONSOLIDA√á√ÉO DOS DADOS
    if not todos_dados:
        print("‚ùå Nenhum dado foi processado com sucesso.")
        return

    print("\nüîÑ Consolidando todos os anos...")
    df_final = pd.concat(todos_dados, ignore_index=True)
    print(f"‚úÖ Dados consolidados: {len(df_final):,} registros")

    # 3. GERA√á√ÉO DO HASH ID_PEDIDO
    print("\nüîë Gerando ID √∫nico para cada pedido...")
    df_final = gerar_id_pedido(df_final)

    # 4. Salvar arquivo consolidado
    print(f"üíæ SALVANDO ARQUIVO CONSOLIDADO...")
    caminho_saida = os.path.join(pasta_outputs, "compras_consolidado_final.csv")
    df_final.to_csv(caminho_saida, index=False, encoding='utf-8-sig', sep=';')
    print(f"‚úÖ Arquivo consolidado salvo em: {caminho_saida}")

    # 5. MODELAGEM DIMENSIONAL
    print("\n[PASSO 5] Modelagem Dimensional (Dimens√µes e Tabela Fato)...")
    df_fato = criar_e_integrar_dimensoes(df_final, pasta_outputs) 
    print(f"‚úÖ Modelagem Dimensional conclu√≠da. Tabela Fato: {len(df_fato):,} registros.")
    
    # =====================================================================
    # üü¢ FASE DE ENRIQUECIMENTO DE DADOS (Risco e Demanda)
    # =====================================================================

    # 6. Risco de Pre√ßo (Z-Score)
    # Esta coluna ('score_z_risco') √© a base para o √çndice de Prioriza√ß√£o (Passo 7).
    print("\n[PASSO 6] C√°lculo do Z-Score de Risco de Pre√ßo...")
    df_fato = calcular_zscore_risco(df_fato)
    print(f"‚úÖ Z-Score de Risco calculado.")

    # 7. C√ÅLCULO DO √çNDICE DE PRIORIZA√á√ÉO
    # Usa 'score_z_risco' e cria as colunas 'demanda_valor' e 'indice_priorizacao'.
    print("\n[PASSO 7] C√°lculo do √çndice de Prioriza√ß√£o de Compras...")
    df_fato = calcular_indice_priorizacao(df_fato)
    print(f"‚úÖ √çndice de Prioriza√ß√£o e 'demanda_valor' calculados.")

    # 8. Risco de Intermit√™ncia (Instabilidade da Demanda)
    # Corrigido: Removida a duplica√ß√£o e mantida uma √∫nica chamada.
    print("\nüü¢ PASSO [8]: C√°lculo de Risco de Intermit√™ncia (Demanda)...")
    df_fato = calcular_risco_intermitencia(df_fato) 
    print("‚úÖ Risco de Intermit√™ncia adicionado.")

    # 9. Risco de Concentra√ß√£o de Fornecedor
    print("\nüü¢ PASSO [9]: C√°lculo de Concentra√ß√£o de Fornecedor (Depend√™ncia)...")
    df_fato = calcular_concentracao_fornecedor(df_fato)
    print("‚úÖ Risco de Concentra√ß√£o adicionado.")

    # 10. (Antigo 11.) TABELA RADAR
    print("\nüéØ Gerando Mini Tabela Fato para o Radar de Oportunidades...")
    df_radar = gerar_mini_fato_radar_enriquecida(df_fato)
                    
    if not df_radar.empty:
        arquivo_radar = os.path.join(pasta_outputs, "mini_fato_radar_oportunidades.csv")
        df_radar.to_csv(arquivo_radar, sep=';', index=False, encoding='utf-8-sig')
        print(f"‚úÖ Mini Fato Radar exportada para: {arquivo_radar}")
    else:
        print("‚ö†Ô∏è A Mini Fato Radar est√° vazia. Verifique os filtros de PMP/Qtd.")
        
    # 11. EXPORTA√á√ÉO FINAL DA TABELA FATO
    print("\n[PASSO 9] Exportando Tabela Fato Final...")
    arquivo_fato = os.path.join(pasta_outputs, "fato_compras_medicamentos.csv")
    df_fato.to_csv(arquivo_fato, index=False, sep=';', encoding='utf-8-sig')
    print(f"‚úÖ Tabela Fato exportada: {arquivo_fato}")

    # 12. ESTAT√çSTICAS E RELAT√ìRIO FINAL
    print(f"\nüéâ PROCESSAMENTO CONCLU√çDO!")
    print(f"   üìà Total de registros: {len(df_final):,}")

    # 13. Estat√≠sticas b√°sicas
    if 'preco_total' in df_final.columns:
        total_gasto = df_final['preco_total'].sum()
        print(f"   üí∞ Total gasto: R$ {total_gasto:,.2f}")
        
        # Gastos por ano
        if 'ano_compra' in df_final.columns:
            gastos_por_ano = df_final.groupby('ano_compra')['preco_total'].sum()
            print(f"   üìä Gastos por ano:")
            for ano, gasto in gastos_por_ano.items():
                print(f"      {ano}: R$ {gasto:,.2f}")

        anos_unicos = sorted([a for a in set(anos_processados) if a != 'desconhecido'])
        print(f"   üìÖ Anos processados: {anos_unicos}")

        if 'uf' in df_final.columns:
            print(f"   üìç Estados participantes: {df_final['uf'].nunique()}")

        if 'descricao_catmat' in df_final.columns:
            print(f"   üíä Medicamentos diferentes: {df_final['descricao_catmat'].nunique()}")

    # 14. Resumo executivo final
        print(f"\n" + "=" * 50)
        print(f"üéØ RESUMO EXECUTIVO FINAL")
        print(f"=" * 50)
    
        if anos_unicos:
            print(f"üìÖ Per√≠odo: {min(anos_unicos)} a {max(anos_unicos)}")
        
            print(f"üìà Total de registros: {len(df_final):,}")
        
        if 'preco_total' in df_final.columns:
            print(f"üí∞ Gasto total: R$ {total_gasto:,.2f}")
        if anos_unicos:
             print(f"üìä M√©dia anual: R$ {total_gasto/len(anos_unicos):,.2f}")
    
        if 'uf' in df_final.columns:
            print(f"üìç Estados: {df_final['uf'].nunique()}")
    
        if 'municipio_instituicao' in df_final.columns:
            print(f"üè¢ Munic√≠pios: {df_final['municipio_instituicao'].nunique()}")
    
        if 'descricao_catmat' in df_final.columns:
            print(f"üíä Medicamentos: {df_final['descricao_catmat'].nunique()}")

        print(f"\nüöÄ PIPELINE COMPLETADO COM SUCESSO!")


def processar_apenas_analises():
    """
    Fun√ß√£o para processar apenas as an√°lises se os dados j√° estiverem consolidados
    """
    print("üîç PROCURANDO DADOS CONSOLIDADOS PARA AN√ÅLISE...")
    
    pasta_base = os.path.dirname(os.path.abspath(__file__))
    pasta_outputs = os.path.join(pasta_base, "data", "outputs")
    arquivo_consolidado = os.path.join(pasta_outputs, "compras_consolidado_final.csv")
    
    if not os.path.exists(arquivo_consolidado):
        print(f"‚ùå Arquivo consolidado n√£o encontrado: {arquivo_consolidado}")
        print("   Execute primeiro o pipeline completo com: python main.py")
        return
    
    try:
        df_final = pd.read_csv(arquivo_consolidado, sep=';', encoding='utf-8-sig')
        print(f"‚úÖ Dados carregados: {len(df_final):,} registros")
                        
        print(f"‚úÖ AN√ÅLISES GERADAS COM SUCESSO!")
        
    except Exception as e:
        print(f"‚ùå Erro ao processar an√°lises: {e}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Pipeline ETL - Compras P√∫blicas de Medicamentos')
    parser.add_argument('--apenas-analises', action='store_true', 
                       help='Executa apenas as an√°lises (sem reprocessar dados)')
    
    args = parser.parse_args()
    
    if args.apenas_analises:
        processar_apenas_analises()
    else:
        main()
