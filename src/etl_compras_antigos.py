import os
import pandas as pd
import logging
import re
import traceback # Para registrar o erro completo

# Configura√ß√£o de Log
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')

class ETLComprasAntigos:
    """
    Pipeline de ETL para dados de compras p√∫blicas de anos antigos (2020-2022)
    VERS√ÉO COMPLETA E CORRIGIDA
    """    
    def __init__(self, pasta_base):
        self.pasta_base = pasta_base # Usa o caminho passado pelo main.py
        self.pasta_raw = os.path.join(self.pasta_base, "raw")
        self.pasta_processed = os.path.join(self.pasta_base, "processed")
        
        print(f"ETL Antigos - Usando caminho base: {self.pasta_base}")
        print(f"Raw: {self.pasta_raw}")
        print(f"Processed: {self.pasta_processed}")

        # Garante pastas
        os.makedirs(self.pasta_raw, exist_ok=True)
        os.makedirs(self.pasta_processed, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        # Garante que o handler do logger est√° configurado
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)

    # --- M√âTODOS DE TRANSFORMA√á√ÉO ESPEC√çFICOS PARA ANOS ANTIGOS ---
    
    # Padroniza nomes de colunas de ANOS ANTIGOS para snake_case do NOVO formato.    
    def _padronizar_colunas(self, df): 
        self.logger.info("üîß Padronizando nomes de colunas...")
        
        # Lista de colunas a serem limpas (removendo espa√ßos e caracteres extras)
        df.columns = df.columns.str.strip().str.replace(' ', '_').str.replace('__', '_')

        map_colunas = {
        'Ano': 'ano_compra',
        'C√≥digo_BR': 'codigo_br',
        'Descri√ß√£o¬†CATMAT': 'descricao_catmat',
        
        
        # CORRE√á√ÉO 1: Mapear para o nome final desejado 'unidade_fornecimento'
        ' Unidade_de_Fornecimento ': 'unidade_fornecimento', # Nome original com espa√ßos
        'Unidade_de_Fornecimento': 'unidade_fornecimento',   # Prote√ß√£o para nome sem espa√ßos
            'Gen√©rico': 'generico',
            'Anvisa': 'anvisa',
            'Compra': 'compra',
            'Modalidade_da_Compra': 'modalidade_compra',
            'Inser√ß√£o': 'insercao',
            'Tipo_Compra': 'tipo_compra',
            
            # Invers√µes de CNPJ/Nome (mantidas como est√£o, pois voc√™ tem l√≥gica para corrigir depois)
            'Fabricante': 'cnpj_fabricante_temp',
            'CNPJ_Fabricante': 'fabricante_temp',
            'Fornecedor': 'cnpj_fornecedor_temp',
            'CNPJ_Fornecedor': 'fornecedor_temp',
            'Nome_Institui√ß√£o': 'cnpj_instituicao_temp',
            'CNPJ_Institui√ß√£o': 'nome_instituicao_temp',
            
            'Munic√≠pio_Institui√ß√£o': 'municipio_instituicao',
            'UF': 'uf',
            'Qtd_Itens_Comprados': 'qtd_itens_comprados',
            'Pre√ßo_Unit√°rio': 'preco_unitario',
            
            # Se 'unidade_fornecimento_capacidade' j√° veio como coluna vazia, n√£o a mapeie.
        }
        
        df.rename(columns=map_colunas, inplace=True)
        self.logger.info("Colunas padronizadas.")
        return df

    # Corrige a invers√£o de CNPJ e Nome que acontece em alguns anos antigos.
    def _corrigir_colunas_trocadas(self, df):
        
        self.logger.info("üîß Corrigindo colunas CNPJ/Nome trocadas...")
        
        # L√≥gica de amostra para determinar se a coluna temp_cnpj realmente √© o nome (se tem mais letras)
        def is_name(series):
            # Conta o n√∫mero de caracteres alfab√©ticos na amostra
            amostra = series.head(100).astype(str).str.replace(r'\W', '', regex=True)
            letras = amostra.str.count(r'[a-zA-Z]').sum()
            digitos = amostra.str.count(r'[0-9]').sum()
            return letras > digitos * 0.5 # Se tiver mais letras do que metade dos d√≠gitos, √© nome

        # Fornecedor
        if 'cnpj_fornecedor_temp' in df.columns and 'fornecedor_temp' in df.columns:
            if is_name(df['cnpj_fornecedor_temp']):
                self.logger.info("   ‚ö†Ô∏è Invers√£o de Fornecedor detectada e corrigida.")
                df['fornecedor'] = df['cnpj_fornecedor_temp']
                df['cnpj_fornecedor'] = df['fornecedor_temp']
            else:
                df['fornecedor'] = df['fornecedor_temp']
                df['cnpj_fornecedor'] = df['cnpj_fornecedor_temp']
            df.drop(columns=['cnpj_fornecedor_temp', 'fornecedor_temp'], inplace=True)
        
        # Fabricante
        if 'cnpj_fabricante_temp' in df.columns and 'fabricante_temp' in df.columns:
            if is_name(df['cnpj_fabricante_temp']):
                self.logger.info("   ‚ö†Ô∏è Invers√£o de Fabricante detectada e corrigida.")
                df['fabricante'] = df['cnpj_fabricante_temp']
                df['cnpj_fabricante'] = df['fabricante_temp']
            else:
                df['fabricante'] = df['fabricante_temp']
                df['cnpj_fabricante'] = df['cnpj_fabricante_temp']
            df.drop(columns=['cnpj_fabricante_temp', 'fabricante_temp'], inplace=True)
            
        # Institui√ß√£o
        if 'cnpj_instituicao_temp' in df.columns and 'nome_instituicao_temp' in df.columns:
            if is_name(df['cnpj_instituicao_temp']):
                self.logger.info("   ‚ö†Ô∏è Invers√£o de Institui√ß√£o detectada e corrigida.")
                df['nome_instituicao'] = df['cnpj_instituicao_temp']
                df['cnpj_instituicao'] = df['nome_instituicao_temp']
            else:
                df['nome_instituicao'] = df['nome_instituicao_temp']
                df['cnpj_instituicao'] = df['cnpj_instituicao_temp']
            df.drop(columns=['cnpj_instituicao_temp', 'nome_instituicao_temp'], inplace=True)
            
        self.logger.info("Colunas CNPJ/Nome ajustadas.")
        return df

    # Corrige tipos de dados com limpeza robusta para ANOS ANTIGOS
    def _corrigir_tipos(self, df):        
        self.logger.info("Corrigindo tipos de dados (Antigos) com limpeza robusta...")

        # 1. CORRE√á√ÉO NUM√âRICA ROBUSTA (Qtd e Pre√ßo)
        colunas_numericas = ['qtd_itens_comprados', 'preco_unitario']
        for coluna in colunas_numericas:
            if coluna in df.columns:
                try:
                    # Passo 1: Limpeza da string para formato regional brasileiro
                    # Remove R$, par√™nteses e espa√ßos.
                    df[coluna] = df[coluna].astype(str).str.replace(r'[R$()+\s]', '', regex=True)
                    # Remove separadores de milhares (ponto)
                    df[coluna] = df[coluna].str.replace('.', '', regex=False)
                    # Troca separador decimal (v√≠rgula) por ponto
                    df[coluna] = df[coluna].str.replace(',', '.', regex=False)
                    
                    # Passo 2: Convers√£o para num√©rico. 'errors=coerce' transforma falhas em NaN.
                    df[coluna] = pd.to_numeric(df[coluna], errors='coerce')

                    # Passo 3: Preenche NaN com 0 para garantir o c√°lculo e evitar erros.
                    df[coluna] = df[coluna].fillna(0)
                    self.logger.info(f"{coluna}: convertido para num√©rico (Limpeza regional aplicada)")

                except Exception as e:
                    self.logger.warning(f"{coluna}: erro na convers√£o num√©rica - {e}")

        # Rec√°lculo de preco_total (necess√°rio ap√≥s a corre√ß√£o dos componentes)
        if 'preco_unitario' in df.columns and 'qtd_itens_comprados' in df.columns:
            df['preco_total'] = df['qtd_itens_comprados'] * df['preco_unitario']
            self.logger.info("preco_total recalculado.")
        else:
            self.logger.warning("N√£o foi poss√≠vel calcular preco_total. Colunas ausentes.")


        # 2. CORRE√á√ÉO DE DATAS
        colunas_data = ['compra', 'insercao']
        for coluna in colunas_data:
            if coluna in df.columns:
                # Usa dayfirst=True para garantir o formato DD/MM/AA (DD/MM/YYYY)
                df[coluna] = pd.to_datetime(df[coluna], errors='coerce', dayfirst=True)
                self.logger.info(f"{coluna}: convertido para data (dayfirst=True)")
                
        
        # 3. CORRE√á√ÉO DE TIPO DE COMPRA (Padroniza para ADMINISTRATIVA/JUDICIAL em CAIXA ALTA)
        if 'tipo_compra' in df.columns:
            df['tipo_compra'] = (
                df['tipo_compra']
                .astype(str)
                .str.strip()
                .str.upper() # Garante caixa alta
                .replace({
                    'A': 'ADMINISTRATIVA',
                    'J': 'JUDICIAL',
                    # Mapeamento para garantir que se vierem por extenso/min√∫sculo, fiquem em caixa alta
                    'ADMINISTRATIVA': 'ADMINISTRATIVA',
                    'JUDICIAL': 'JUDICIAL',
                })
            )
            
            # Trata valores nulos ou inesperados
            valores_validos = ['ADMINISTRATIVA', 'JUDICIAL']
            df['tipo_compra'] = df['tipo_compra'].apply(
                lambda x: 'INDEFINIDO' if x not in valores_validos else x
            )

            self.logger.info(f"tipo_compra: padronizado (ADMINISTRATIVA/JUDICIAL).")


        # 4. PADRONIZA√á√ÉO DO C√ìDIGO BR (Remo√ß√£o do prefixo 'BR')
        if 'codigo_br' in df.columns:
            self.logger.info("codigo_br: Removendo prefixo 'BR'...")
            
            # 1. Converte para string e remove espa√ßos em branco
            df['codigo_br'] = df['codigo_br'].astype(str).str.strip()
            
            # 2. Usa uma express√£o regular ou .str.replace() para remover 'BR' se estiver no in√≠cio
            # O .str.replace √© mais simples para este caso:
            df['codigo_br'] = (
                df['codigo_br']
                .str.upper() # Coloca em caixa alta para pegar "br" ou "Br"
                .str.replace(r'^BR0*', '', regex=True) # Remove BR e qualquer zero √† esquerda subsequente (se for o caso)
            )
                        
            # Garante que o valor resultante seja um n√∫mero (em string) sem 'BR' e sem espa√ßos.
            def limpar_codigo_br(codigo):
                if pd.isna(codigo):
                    return None
                # Remove 'BR' e qualquer caractere n√£o-num√©rico
                codigo_limpo = re.sub(r'^BR', '', str(codigo).strip().upper()) 
                # Garante que n√£o haja espa√ßo no final
                return codigo_limpo.strip() 
            
            df['codigo_br'] = df['codigo_br'].apply(limpar_codigo_br)

            self.logger.info(f"codigo_br: prefixo 'BR' removido e padronizado.")

        
        # 5. CORRE√á√ÉO CNPJ/FLAG/STRING
        # CNPJs
        colunas_cnpj = ['cnpj_instituicao', 'cnpj_fornecedor', 'cnpj_fabricante']
        for coluna in colunas_cnpj:
            if coluna in df.columns:
                df[coluna] = df[coluna].astype(str).str.replace(r'\D', '', regex=True).str.zfill(14)
                self.logger.info(f"{coluna}: limpeza e zfill(14)")

        
        # 6. Flags (Generico)
        if 'generico' in df.columns:
            df['generico'] = (
                df['generico']
                .astype(str)
                .str.strip()
                .str.upper() # Garante que "Sim" e "sim" virem "SIM"
                .replace({'N√ÉO': 'N√ÉO', 'NAO': 'N√ÉO'}) # Garante a acentua√ß√£o
                .fillna('N√ÉO')
            )
            # Qualquer valor que ainda seja 'S' ou 'N' de um poss√≠vel erro anterior ser√° corrigido aqui
            df['generico'] = df['generico'].replace({'S': 'SIM', 'N': 'N√ÉO'})
            
            # Finalmente, qualquer valor que n√£o seja SIM ou N√ÉO √© for√ßado para N√ÉO
            df['generico'] = df['generico'].apply(lambda x: 'N√ÉO' if x not in ['SIM', 'N√ÉO'] else x)

            self.logger.info(f"generico: padronizado (SIM/N√ÉO) (Antigos anos).")

        # 7. Colunas de texto (limpeza b√°sica)
        colunas_string = ['nome_instituicao', 'municipio_instituicao', 'uf',
                          'fornecedor', 'fabricante', 'descricao_catmat',
                          'unidade_fornecimento_capacidade', 'modalidade_compra', 'tipo_compra', 'anvisa', 'codigo_br']
        for coluna in colunas_string:
            if coluna in df.columns:
                df[coluna] = df[coluna].astype(str).str.strip().replace('nan', '').replace('None', '')
        
        # O campo anvisa deve ser tratado como string para evitar perdas de leading zeros.
        if 'anvisa' in df.columns:
            df['anvisa'] = df['anvisa'].str.replace(r'\D', '', regex=True)

        self.logger.info("Tipos corrigidos com sucesso.")
        return df
    
    # Adiciona colunas que n√£o existem nos anos antigos com valores padr√£o
    def _adicionar_colunas_vazias(self, df):        
        self.logger.info("Adicionando colunas ausentes (capacidade, unidade_medida)...")
        colunas_novas = {
            'capacidade': 0.0,
            'unidade_medida': 'NA',
        }
        for col, default in colunas_novas.items():
            if col not in df.columns:
                df[col] = default
        self.logger.info("Colunas ausentes adicionadas.")
        return df

    # Garante que a ordem das colunas seja igual ao formato NOVO para consolida√ß√£o.
    def _reordenar_colunas(self, df):        
        self.logger.info("Reordenando colunas...")
        colunas_finais = [
            'ano_compra', 'nome_instituicao', 'cnpj_instituicao', 'municipio_instituicao',
            'uf', 'compra', 'insercao', 'codigo_br', 'descricao_catmat',
            'unidade_fornecimento_capacidade', 'generico', 'anvisa', 'modalidade_compra',
            'tipo_compra', 'capacidade', 'unidade_medida', 'cnpj_fornecedor',
            'fornecedor', 'cnpj_fabricante', 'fabricante', 'qtd_itens_comprados',
            'preco_unitario', 'preco_total'
        ]
        
        # Filtra apenas as colunas que realmente existem no DataFrame
        colunas_presentes = [col for col in colunas_finais if col in df.columns]
        
        # Adiciona colunas presentes que n√£o foram mapeadas na ordem, no final
        for col in df.columns:
            if col not in colunas_presentes:
                colunas_presentes.append(col)
        
        df = df[colunas_presentes]
        self.logger.info("Colunas reordenadas.")
        return df

    # Executa a sequ√™ncia de transforma√ß√µes para um √∫nico DataFrame antigo.
    def _processar_arquivo(self, df):        
        df = self._padronizar_colunas(df)
        df = self._corrigir_colunas_trocadas(df)
        df = self._corrigir_tipos(df)
        df = self._adicionar_colunas_vazias(df)
        df = self._reordenar_colunas(df)
        return df

    # --- METODOS DE I/O E EXECU√á√ÉO ---

    # Lista arquivos antigos com correspond√™ncia exata de nome YYYY.csv
    def listar_arquivos_antigos(self):        
        if not os.path.exists(self.pasta_raw):
            self.logger.error("Pasta raw n√£o existe!")
            return []
            
        todos_arquivos = os.listdir(self.pasta_raw)
        arquivos_antigos = []
        ANOS_ANTIGOS = ['2020', '2021', '2022']
        
        for f in todos_arquivos:
            # Verifica se √© um CSV e se o nome do arquivo (sem extens√£o) est√° na lista de anos antigos
            nome_base, ext = os.path.splitext(f)
            if ext.lower() == '.csv' and nome_base in ANOS_ANTIGOS:
                caminho = os.path.join(self.pasta_raw, f)
                arquivos_antigos.append(caminho)
                self.logger.info(f"Arquivo antigo encontrado: {f}")

        self.logger.info(f"Total arquivos antigos a processar: {len(arquivos_antigos)}")
        return arquivos_antigos

    # Processa todos os arquivos antigos (2020-2022) em lote, com tratamento de erro individual. 
    def processar_todos_antigos(self):       
        self.logger.info("Iniciando processamento de todos os anos antigos (2020-2022)...")
        arquivos = self.listar_arquivos_antigos()
        dfs = []
        anos_processados = []

        for arquivo_path in arquivos:
            nome_arquivo = os.path.basename(arquivo_path)
            self.logger.info(f"\n>>INICIANDO processamento do arquivo: {nome_arquivo}")

            df = None
            encoding_tentativas = ['utf-8-sig', 'latin-1', 'iso-8859-1'] # Ordem de prefer√™ncia

            for encoding in encoding_tentativas:
                try:
                    # Tenta ler com a codifica√ß√£o atual
                    df = pd.read_csv(arquivo_path, sep=';', encoding=encoding, low_memory=False)
                    self.logger.info(f"Leitura bem-sucedida com encoding: {encoding}")
                    break # Se leu, sai do loop de tentativas
                
                except UnicodeDecodeError:
                    # Se falhou por causa de codifica√ß√£o, tenta a pr√≥xima
                    self.logger.warning(f"Falha na leitura com encoding '{encoding}'. Tentando o pr√≥ximo...")
                
                except pd.errors.EmptyDataError:
                    self.logger.error(f"Erro de Dados: Arquivo vazio ou ileg√≠vel: {nome_arquivo}")
                    break # Se o erro n√£o for de codifica√ß√£o, quebra o loop de tentativas
                
                except Exception as e:
                    # Captura qualquer outro erro que n√£o seja de codifica√ß√£o na leitura
                    self.logger.error(f"ERRO GRAVE inesperado na leitura de {nome_arquivo}: {e}")
                    self.logger.error(f"Detalhes do Erro:\n{traceback.format_exc()}")
                    break

            if df is None or df.empty:
                self.logger.error(f"Processamento ABORTADO para {nome_arquivo}. N√£o foi poss√≠vel ler o arquivo com as codifica√ß√µes tentadas.")
                continue # Pula para o pr√≥ximo arquivo no loop principal

            # Continua√ß√£o do processamento SE o DataFrame foi lido com sucesso
            try:
                # Processamento (Onde ocorrem as corre√ß√µes de tipos, padroniza√ß√£o, etc.)
                df_processado = self._processar_arquivo(df)
                
                dfs.append(df_processado)
                
                # Extrai o ano
                ano = os.path.splitext(nome_arquivo)[0]
                anos_processados.append(ano)
                self.logger.info(f"FINALIZADO com sucesso: {nome_arquivo} ({len(df_processado):,} registros)")
            
            except Exception as e:
                # Captura erros que ocorrem *ap√≥s* a leitura (ex: convers√£o de tipos, limpeza)
                self.logger.error(f"ERRO GRAVE na transforma√ß√£o de {nome_arquivo}. O arquivo ser√° pulado.")
                self.logger.error(f"Mensagem do Erro: {e}")
                self.logger.error(f"Detalhes (Traceback):\n{traceback.format_exc()}")

        
        if not dfs:
            self.logger.warning("\nNenhum DataFrame foi processado com sucesso para consolidar.")
            return None

        self.logger.info("\nTentando consolidar todos os DataFrames processados...")

        df_consolidado = pd.concat(dfs, ignore_index=True)
        
        # Salvamento
        anos_str = "_".join(sorted(set(anos_processados)))
        caminho_consolidado = os.path.join(self.pasta_processed, f"compras_antigos_consolidado_{anos_str}.csv")
        df_consolidado.to_csv(caminho_consolidado, index=False, encoding='utf-8-sig', sep=';')
        
        self.logger.info(f"Consolida√ß√£o completa!")
        self.logger.info(f"Total de registros: {len(df_consolidado):,}")
        self.logger.info(f"Anos: {', '.join(sorted(set(anos_processados))) if anos_processados else 'N/A'}")
        self.logger.info(f"Arquivo: {caminho_consolidado}")
        
        return df_consolidado


# Fun√ß√£o de conveni√™ncia
# Executa o pipeline completo para anos antigos
def processar_anos_antigos():    
    # Note que o construtor do ETLComprasAntigos usa o caminho fixo
    etl_antigo = ETLComprasAntigos("") 
    return etl_antigo.processar_todos_antigos()

if __name__ == "__main__":
    print("=" * 60)
    print("ETL - COMPRAS P√öBLICAS ANOS ANTIGOS (2020-2022)")
    print("=" * 60)
    
    try:
        df_antigos = processar_anos_antigos()
        
        if df_antigos is not None:
            print(f"\nPROCESSAMENTO DE ANOS ANTIGOS CONCLU√çDO! {len(df_antigos):,} registros consolidados.")
        else:
            print("\n O pipeline foi conclu√≠do, mas nenhum dado p√¥de ser consolidado.")
            
    except Exception as e:
        print(f"\nERRO FATAL NO MAIN: {e}")
        print(f"Detalhes: {traceback.format_exc()}")